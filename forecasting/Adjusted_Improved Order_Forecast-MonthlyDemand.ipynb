{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e563e79c-3d4a-4e80-b3ad-ad87241a8fb7",
   "metadata": {},
   "source": [
    "# Applying Various ML Models (RNN, LSTM, CNN, CNN→RNN, CNN→LSTM) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbeb6d8-fb42-4f4c-b3a4-391cb7a2a494",
   "metadata": {},
   "source": [
    "## Step 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9a82142a-2818-41a1-b6a4-edd1ad252b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import tensorflow as tf\n",
    "tf.config.run_functions_eagerly(False)  # ensure graph mode\n",
    "from tensorflow.keras import layers, models, callbacks, optimizers\n",
    "from tensorflow.keras.losses import Huber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "deced3d3-a78b-4bb7-b3d1-f050bd959121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIG (smart) ---\n",
    "# What you want to do\n",
    "FORECAST_MONTHS = 12          # set to 12 if you need 2025M1–M12\n",
    "SEASONAL_PERIOD = 12          # monthly seasonality\n",
    "\n",
    "# Windowing\n",
    "LOOKBACK       = max(SEASONAL_PERIOD, 12)   # at least one seasonal cycle\n",
    "LOOKBACK_PLAN  = LOOKBACK\n",
    "HORIZON        = FORECAST_MONTHS            # align all models to same H\n",
    "HORIZON_PLAN   = HORIZON\n",
    "\n",
    "# Splits / training\n",
    "VAL_FRAC   = 0.20      # chronological validation fraction\n",
    "MIN_TRAIN  = 1         # min train windows safeguard (don’t touch)\n",
    "MIN_VAL    = 1         # min val windows safeguard (don’t touch)\n",
    "BATCH      = 16        # will be clamped to dataset size at runtime\n",
    "EPOCHS     = 200\n",
    "PATIENCE   = 20\n",
    "LR         = 1e-3\n",
    "SEED       = 42\n",
    "\n",
    "# Products (keep in sync with your data)\n",
    "PRODUCTS = [\"iPhone\",\"iPad\",\"MacBook\"]\n",
    "\n",
    "# Tariff/elasticity params (OPTIONAL; used only if you apply tariff layer)\n",
    "ELASTICITY   = {\"iPhone\": -1.2, \"iPad\": -0.8, \"MacBook\": -1.0}   # demand elasticity\n",
    "PASS_THROUGH = {\"iPhone\":  0.7, \"iPad\":  0.6, \"MacBook\":  0.65}  # share of tariff passed to price\n",
    "CHINA_SHARE  = {\"iPhone\":  0.6, \"iPad\":  0.5, \"MacBook\":  0.55}  # input share\n",
    "# Build a horizon-length tariff path (e.g., constant 5% then 10% after month 6)\n",
    "TARIFF_PATH  = {h: (0.05 if h <= 6 else 0.10) for h in range(1, HORIZON+1)}\n",
    "\n",
    "# Seeding (TF + NumPy + Python)\n",
    "import os, random, numpy as np, tensorflow as tf\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "\n",
    "# # ---- Runtime sanity helpers (optional) ----\n",
    "# def _print_windowing_sanity(df):\n",
    "#     \"\"\"Call right after you make df_use. Helps avoid 'too few windows' surprises.\"\"\"\n",
    "#     from math import floor\n",
    "#     d = df.copy()\n",
    "#     # total months per scenario (approx; ignores missing months)\n",
    "#     by_scen = d.groupby(\"scenario\")[[\"Year\",\"period\"]].nunique().assign(\n",
    "#         approx_months=lambda x: x[\"Year\"]*12  # rough, but useful guard\n",
    "#     )\n",
    "#     print(\"\\n[Sanity] Windowing targets\")\n",
    "#     print(f\"  LOOKBACK={LOOKBACK}, HORIZON={HORIZON}, VAL_FRAC={VAL_FRAC}\")\n",
    "#     print(\"[Sanity] Per-scenario rough month counts:\\n\", by_scen[[\"approx_months\"]])\n",
    "\n",
    "def _auto_adjust_for_small_data(n_windows):\n",
    "    \"\"\"Optional: call inside your training script to soften configs when data is tiny.\"\"\"\n",
    "    global VAL_FRAC, BATCH, EPOCHS, PATIENCE\n",
    "    if n_windows < 10:\n",
    "        VAL_FRAC = min(VAL_FRAC, 0.15)   # keep more for train\n",
    "        BATCH    = min(BATCH, max(1, n_windows//2))\n",
    "        EPOCHS   = min(EPOCHS, 150)\n",
    "        PATIENCE = min(PATIENCE, 15)\n",
    "def _norm_prod(name: str) -> str:\n",
    "    return str(name).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "501e5911-8df6-4ca9-8138-0824a8c2174b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_demand_excel(path):\n",
    "    df = pd.read_csv(\n",
    "        path,\n",
    "        dtype={\"period\": int, \"product_id\": int, \"product_name\": str, \"scenario\": str, \"region\":str},\n",
    "    )\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    # parse numbers like \"90,022\" (works even if Excel already loaded them as numeric)\n",
    "    df[\"demand_units\"] = (\n",
    "        df[\"demand_units\"].astype(str).str.replace(\",\", \"\", regex=False).astype(float)\n",
    "    )\n",
    "    df[\"product_name\"] = df[\"product_name\"].str.strip()\n",
    "    df = df.sort_values([\"scenario\", \"product_id\", \"period\",\"region\"]).reset_index(drop=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "be1ab06b-38d6-4e71-a754-0fff5be6d7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _windows_per_scenario(T: int, L: int, H: int, R: int) -> int:\n",
    "    \"\"\"\n",
    "    How many rolling windows can we extract from a time series of length T\n",
    "    after we 'repeat' the series R times (data augmentation),\n",
    "    using lookback L and forecast horizon H?\n",
    "\n",
    "    Formula: max(0, (T*R) - L - H + 1)\n",
    "\n",
    "    Why:\n",
    "      - (T*R) is the effective length after repeating R times.\n",
    "      - For each window, we need L history points and H future points.\n",
    "      - The last valid window starts at index (T*R - L - H).\n",
    "    \"\"\"\n",
    "    return max(0, (T * R) - L - H + 1)\n",
    "\n",
    "\n",
    "# --- uses your existing make_df_use(...) and normalize_time_strict(...) ---\n",
    "\n",
    "def _choose_min_repeat(df, lookback: int, horizon: int,\n",
    "                       val_frac: float = 0.2, min_train_windows: int = 8) -> int:\n",
    "    \"\"\"\n",
    "    Choose the smallest augmentation repeat R (1..20) that yields enough *training* windows.\n",
    "    We compute T per scenario using a continuous month index (t_month), not raw 'period'.\n",
    "    \"\"\"\n",
    "    d = make_df_use(df)\n",
    "    d = normalize_time_strict(d)  # adds t_month per scenario (0,1,2,...)\n",
    "    # number of months (rows) per scenario in time order\n",
    "    T_per_scen = d.groupby(\"scenario\")[\"t_month\"].max().add(1).to_dict()\n",
    "\n",
    "    for R in range(1, 21):\n",
    "        N_tot = sum(_windows_per_scenario(Ts, lookback, horizon, R) for Ts in T_per_scen.values())\n",
    "        n_val = max(1, int(round(N_tot * val_frac))) if N_tot > 1 else 0\n",
    "        n_tr  = N_tot - n_val\n",
    "        if n_tr >= min_train_windows:\n",
    "            return R\n",
    "    return 2  # fallback for very small datasets\n",
    "\n",
    "\n",
    "def make_windows_from_tidy(df, lookback, horizon, augment_repeat=None,\n",
    "                           products=None, val_frac=0.2, min_train_windows=8):\n",
    "    \"\"\"\n",
    "    Build rolling windows on a *rectangular* panel indexed by continuous t_month.\n",
    "    Targets are the first len(PRODUCTS) columns; extras (month one-hots, flags) come after.\n",
    "    \"\"\"\n",
    "    if products is None:\n",
    "        products = PRODUCTS\n",
    "    m_out = len(products)\n",
    "\n",
    "    # auto-choose augmentation\n",
    "    if augment_repeat is None:\n",
    "        augment_repeat = _choose_min_repeat(df, lookback, horizon, val_frac, min_train_windows)\n",
    "\n",
    "    # normalize & time-index\n",
    "    d = make_df_use(df)\n",
    "    d = normalize_time_strict(d)  # t_month added\n",
    "    Xs, ys, per_scen_summary = [], [], []\n",
    "\n",
    "    # build per-scenario rectangular panel on t_month\n",
    "    for scen, g in d.groupby(\"scenario\", sort=True):\n",
    "        # sum regions if any already handled in make_df_use\n",
    "        # pivot by *t_month* to avoid duplicates from repeating calendar months\n",
    "        pv = (g.pivot_table(index=\"t_month\", columns=\"product_name\",\n",
    "                            values=\"demand_units\", aggfunc=\"sum\")\n",
    "                .reindex(columns=products))\n",
    "        # handle gaps\n",
    "        pv = pv.ffill().bfill()  # or pv.fillna(0.0)\n",
    "        # --- exogenous features derived from calendar month (1..12) ---\n",
    "        # month number from (t_month % 12) + 1\n",
    "        month_num = (pv.index.values % 12) + 1\n",
    "        month_oh = pd.get_dummies(month_num)\n",
    "        # consistent names m_01..m_12 (avoid m_2..m_12 vs m_02..m_12 mismatches)\n",
    "        month_oh.columns = [f\"m_{i:02d}\" for i in month_oh.columns]\n",
    "        exo = month_oh.copy()\n",
    "        exo[\"flag_launch\"]  = (month_num == 9).astype(float)\n",
    "        exo[\"flag_holiday\"] = np.isin(month_num, [10, 11, 12]).astype(float)\n",
    "\n",
    "        arr_prod = pv.values.astype(\"float32\")             # (T, m_out)\n",
    "        arr_exo  = exo.values.astype(\"float32\")            # (T, n_exo)\n",
    "        series   = np.concatenate([arr_prod, arr_exo], 1)  # (T, m_out + n_exo)\n",
    "\n",
    "        # repeat contiguously R times (simple augmentation)\n",
    "        series = np.vstack([series for _ in range(augment_repeat)])\n",
    "        T_eff, m_in = series.shape\n",
    "        N_here = T_eff - lookback - horizon + 1\n",
    "        per_scen_summary.append((scen, T_eff, max(0, N_here)))\n",
    "        if N_here <= 0:\n",
    "            continue\n",
    "\n",
    "        for start in range(N_here):\n",
    "            end = start + lookback\n",
    "            Xs.append(series[start:end, :])                         # (L, m_in)\n",
    "            ys.append(series[end:end+horizon, :m_out].reshape(-1))  # (H*m_out,)\n",
    "\n",
    "    if len(Xs) < 2:\n",
    "        diag = \"\\n\".join([f\"  • scen='{s}': T_eff={tt}, windows={wh}\" for s,tt,wh in per_scen_summary])\n",
    "        raise ValueError(\n",
    "            f\"No/too few windows. lookback={lookback}, horizon={horizon}, repeat={augment_repeat}\\n{diag}\"\n",
    "        )\n",
    "\n",
    "    return np.asarray(Xs, dtype=\"float32\"), np.asarray(ys, dtype=\"float32\")\n",
    "\n",
    "\n",
    "def naive_same_month_last_year(df, scenario, horizon, lookback):\n",
    "    \"\"\"\n",
    "    Copy the same calendar month from *last year* using t_month indexing.\n",
    "    If the history is too short, repeat the last 12-month profile.\n",
    "    Returns an array of shape (H, m_out).\n",
    "    \"\"\"\n",
    "    d = make_df_use(df)\n",
    "    d = normalize_time_strict(d)\n",
    "    g = d[d[\"scenario\"] == scenario].copy()\n",
    "    pv = (g.pivot_table(index=\"t_month\", columns=\"product_name\",\n",
    "                        values=\"demand_units\", aggfunc=\"sum\")\n",
    "            .reindex(columns=PRODUCTS)\n",
    "            .sort_index())\n",
    "    pv = pv.ffill().bfill()\n",
    "    mat = pv.values.astype(\"float32\")  # (T, m_out)\n",
    "    T, m = mat.shape\n",
    "    if T < 12 + horizon:\n",
    "        # fallback: tile last 12\n",
    "        last12 = (mat[-12:, :] if T >= 12 else np.tile(mat[-1:, :], (12, 1)))\n",
    "        reps = int(np.ceil(horizon / 12))\n",
    "        return np.vstack([last12] * reps)[:horizon, :]\n",
    "\n",
    "    yhat = []\n",
    "    for h in range(1, horizon + 1):\n",
    "        tgt_idx = T - lookback + h - 1\n",
    "        src_idx = tgt_idx - 12\n",
    "        src_idx = np.clip(src_idx, 0, T - 1)\n",
    "        yhat.append(mat[src_idx, :])\n",
    "    return np.vstack(yhat)\n",
    "\n",
    "# Splits the pooled windows into train/validation by keeping order and taking the last n_val windows as validation.This is time-aware (not random) and avoids shuffling, which is correct for time series.\n",
    "def split_windows(X, y, val_frac=VAL_FRAC, min_train=1, min_val=1):\n",
    "    \"\"\"\n",
    "    Chronological (time-aware) split of pooled windows.\n",
    "    Keeps the last n_val windows for validation.\n",
    "    Guarantees at least `min_train` train and `min_val` validation windows\n",
    "    if possible. If data are too short, still returns valid arrays.\n",
    "    \"\"\"\n",
    "    N = X.shape[0]\n",
    "    if N < 2:\n",
    "        # Not enough windows to split; all go to training\n",
    "        print(f\"[split_windows] Only {N} window(s) available — using all for training.\")\n",
    "        return X, y, X[:0], y[:0]\n",
    "\n",
    "    # Compute validation count safely\n",
    "    n_val = max(min_val, int(round(N * val_frac)))\n",
    "    n_val = min(n_val, N - min_train)  # leave enough for training\n",
    "    if n_val <= 0:\n",
    "        n_val = 1 if N > 1 else 0\n",
    "\n",
    "    # Chronological split (no shuffle)\n",
    "    Xtr, ytr = X[:N - n_val], y[:N - n_val]\n",
    "    Xval, yval = X[N - n_val:], y[N - n_val:]\n",
    "    return Xtr, ytr, Xval, yval\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b366e7b-418b-44d8-9f67-f43218d1a514",
   "metadata": {},
   "source": [
    "## Step 2: Applying DL Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "09953b03-c01c-4d66-b211-62fdb78d013f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Canonical dataframe utilities to standardize schema\n",
    "# ---------------------------------------------------\n",
    "# def make_df_use(df):\n",
    "#     \"\"\"Unify column names; sum regions if present; add default scenario if missing\"\"\"\n",
    "#     Y  = \"Year\" if \"Year\" in df.columns else \"year\"\n",
    "#     P  = \"period\" if \"period\" in df.columns else (\"month\" if \"month\" in df.columns else \"Month\")\n",
    "#     PN = \"product_name\" if \"product_name\" in df.columns else (\"product\" if \"product\" in df.columns else \"product_id\")\n",
    "#     V  = (\"demand_units\" if \"demand_units\" in df.columns else\n",
    "#           (\"demand\" if \"demand\" in df.columns else\n",
    "#            (\"qty\" if \"qty\" in df.columns else \"quantity\")))\n",
    "#     d = df.copy()\n",
    "#     if \"region\" in d.columns:\n",
    "#         d = d.groupby([Y, P, PN], as_index=False)[V].sum()\n",
    "#     if \"scenario\" not in d.columns:\n",
    "#         d[\"scenario\"] = \"Baseline_Seasonal\"\n",
    "#     d = d.rename(columns={Y:\"Year\", P:\"period\", PN:\"product_name\", V:\"demand_units\"})\n",
    "#     d[\"Year\"]   = d[\"Year\"].astype(int)\n",
    "#     d[\"period\"] = d[\"period\"].astype(int)\n",
    "#     return d\n",
    "\n",
    "def make_df_use(df):\n",
    "    \"\"\"Unify column names; sum regions if present; add default scenario if missing\"\"\"\n",
    "    Y  = \"Year\"\n",
    "    P  = \"period\" \n",
    "    PN = \"product_name\"\n",
    "    V  = \"demand_units\"\n",
    "    scenario=\"scenario\"\n",
    "    d = df.copy()\n",
    "    d = d.groupby([Y, P, PN,scenario], as_index=False)[V].sum()\n",
    "    d[\"Year\"]   = d[\"Year\"].astype(int)\n",
    "    d[\"period\"] = d[\"period\"].astype(int)\n",
    "    return d\n",
    "\n",
    "\n",
    "def normalize_time_strict(d, sc=\"scenario\", yc=\"Year\", mc=\"period\"):\n",
    "    \"\"\"Create a continuous month index per scenario (t_month)\"\"\"\n",
    "    d = d.copy()\n",
    "    d[yc] = d[yc].astype(int)\n",
    "    d[mc] = d[mc].astype(int)\n",
    "    months = (d[[sc, yc, mc]].drop_duplicates()\n",
    "              .sort_values([sc, yc, mc]).reset_index(drop=True))\n",
    "    months[\"t_month\"] = months.groupby(sc).cumcount()\n",
    "    return d.merge(months, on=[sc, yc, mc], how=\"left\")\n",
    "\n",
    "def panel_for_scenario_full(df, scen, products=PRODUCTS):\n",
    "    \"\"\"\n",
    "    Build a rectangular (t_month × product) panel for one scenario.\n",
    "    Works whether df already has t_month or only Year/period.\n",
    "    \"\"\"\n",
    "    d = make_df_use(df)  # ensures Year/period/product_name/demand_units/scenario\n",
    "    if \"t_month\" not in d.columns:\n",
    "        d = normalize_time_strict(d)   # create t_month from Year/period per scenario\n",
    "\n",
    "    g = d[d[\"scenario\"] == scen].copy()\n",
    "    if g.empty:\n",
    "        raise ValueError(f\"Scenario '{scen}' not found in dataframe.\")\n",
    "\n",
    "    # guard against dup rows for same (t_month, product)\n",
    "    g = (g.groupby([\"t_month\", \"product_name\"], as_index=False)[\"demand_units\"]\n",
    "           .sum())\n",
    "\n",
    "    pv = (g.pivot(index=\"t_month\", columns=\"product_name\", values=\"demand_units\")\n",
    "            .reindex(columns=products)\n",
    "            .sort_index())\n",
    "\n",
    "    # keep panel rectangular for windowing\n",
    "    pv = pv.ffill()\n",
    "    return pv\n",
    "\n",
    "\n",
    "# -------------------\n",
    "# Model architectures\n",
    "# -------------------\n",
    "def build_rnn(L, m, H, width=64, depth=1, dropout=0.2):\n",
    "    x = layers.Input(shape=(L, m))\n",
    "    h = x\n",
    "    for _ in range(depth):\n",
    "        h = layers.SimpleRNN(width, return_sequences=True)(h)\n",
    "        h = layers.Dropout(dropout)(h)\n",
    "    h = layers.Flatten()(h)\n",
    "    out = layers.Dense(H * len(PRODUCTS))(h)\n",
    "    model = models.Model(x, out)\n",
    "    model.compile(optimizer=optimizers.Adam(LR), loss=Huber(delta=1.0))\n",
    "    return model\n",
    "\n",
    "def build_lstm(L, m, H, width=64, depth=1, dropout=0.2):\n",
    "    x = layers.Input(shape=(L, m))\n",
    "    h = x\n",
    "    for i in range(depth):\n",
    "        h = layers.LSTM(width, return_sequences=(i < depth-1))(h)\n",
    "        h = layers.Dropout(dropout)(h)\n",
    "    if depth == 1:\n",
    "        h = layers.Flatten()(layers.RepeatVector(1)(h))\n",
    "    else:\n",
    "        h = layers.Flatten()(h)\n",
    "    out = layers.Dense(H * len(PRODUCTS))(h)\n",
    "    model = models.Model(x, out)\n",
    "    model.compile(optimizer=optimizers.Adam(LR), loss=Huber(delta=1.0))\n",
    "    return model\n",
    "\n",
    "def build_cnn(L, m, H, width=64, depth=2, dropout=0.2):\n",
    "    x = layers.Input(shape=(L, m))\n",
    "    h = x\n",
    "    for _ in range(depth):\n",
    "        h = layers.Conv1D(filters=width, kernel_size=3, padding=\"causal\", activation=\"relu\")(h)\n",
    "        h = layers.Dropout(dropout)(h)\n",
    "    h = layers.GlobalAveragePooling1D()(h)\n",
    "    out = layers.Dense(H * len(PRODUCTS))(h)\n",
    "    model = models.Model(x, out)\n",
    "    model.compile(optimizer=optimizers.Adam(LR), loss=Huber(delta=1.0))\n",
    "    return model\n",
    "\n",
    "def build_cnn_rnn(L, m, H, width=64, depth_cnn=2, rnn_units=64, dropout=0.2):\n",
    "    x = layers.Input(shape=(L, m))\n",
    "    h = x\n",
    "    for _ in range(depth_cnn):\n",
    "        h = layers.Conv1D(filters=width, kernel_size=3, padding=\"causal\", activation=\"relu\")(h)\n",
    "        h = layers.Dropout(dropout)(h)\n",
    "    h = layers.LSTM(rnn_units)(h)\n",
    "    out = layers.Dense(H * len(PRODUCTS))(h)\n",
    "    model = models.Model(x, out)\n",
    "    model.compile(optimizer=optimizers.Adam(LR), loss=Huber(delta=1.0))\n",
    "    return model\n",
    "\n",
    "def build_cnn_lstm(L, m, H, width=64, depth_cnn=2, lstm_units=64, dropout=0.2):\n",
    "    x = layers.Input(shape=(L, m))\n",
    "    h = x\n",
    "    for _ in range(depth_cnn):\n",
    "        h = layers.Conv1D(filters=width, kernel_size=5, padding=\"causal\", activation=\"relu\")(h)\n",
    "        h = layers.MaxPooling1D(pool_size=2)(h)\n",
    "        h = layers.Dropout(dropout)(h)\n",
    "    h = layers.LSTM(lstm_units)(h)\n",
    "    out = layers.Dense(H * len(PRODUCTS))(h)\n",
    "    model = models.Model(x, out)\n",
    "    model.compile(optimizer=optimizers.Adam(LR), loss=Huber(delta=1.0))\n",
    "    return model\n",
    "\n",
    "MODEL_BUILDERS = {\n",
    "    \"rnn\":       build_rnn,\n",
    "    \"lstm\":      build_lstm,\n",
    "    \"cnn\":       build_cnn,\n",
    "    \"cnn_rnn\":   build_cnn_rnn,\n",
    "    \"cnn_lstm\":  build_cnn_lstm,\n",
    "}\n",
    "\n",
    "# ------------------\n",
    "# Error metrics\n",
    "# ------------------\n",
    "def rmse_vec(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=float).ravel()\n",
    "    y_pred = np.asarray(y_pred, dtype=float).ravel()\n",
    "    return float(np.sqrt(np.mean((y_true - y_pred)**2)))\n",
    "\n",
    "def smape_vec(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    denom  = (np.abs(y_true) + np.abs(y_pred)) / 2.0\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        frac = np.abs(y_true - y_pred) / np.maximum(denom, 1e-12)\n",
    "    return float(np.nanmean(frac) * 100.0)\n",
    "\n",
    "\n",
    "# ---------- SAFE SPLITTER ----------\n",
    "def split_windows_safe(X, y, val_frac=0.2, min_train=1, min_val=1):\n",
    "    \"\"\"Chronological split with safety for tiny datasets.\"\"\"\n",
    "    N = X.shape[0]\n",
    "    if N < min_train + min_val:\n",
    "        if N >= 2:\n",
    "            split = N - 1\n",
    "            Xtr, ytr = X[:split], y[:split]\n",
    "            Xval, yval = X[split:], y[split:]\n",
    "        else:\n",
    "            Xtr, ytr = X, y\n",
    "            Xval, yval = X[:0], y[:0]\n",
    "        return Xtr, ytr, Xval, yval\n",
    "    split = max(min_train, int(round(N * (1.0 - val_frac))))\n",
    "    split = min(split, N - min_val)\n",
    "    return X[:split], y[:split], X[split:], y[split:]\n",
    "\n",
    "# ---------- UPDATED MODEL ZOO ----------\n",
    "def run_model_zoo(df, lookback=LOOKBACK, horizon=HORIZON, augment_repeat=None, USE_Y_SCALING=True):\n",
    "    X_raw, y_raw = make_windows_from_tidy(df, lookback, horizon, augment_repeat)  # X:(N,L,m_in)  y:(N,H*m_out)\n",
    "    L, m_in = X_raw.shape[1], X_raw.shape[2]\n",
    "    m_out   = len(PRODUCTS)\n",
    "    assert y_raw.shape[1] % m_out == 0, \"y must be shaped (N, H*m_out)\"\n",
    "    H  = y_raw.shape[1] // m_out\n",
    "\n",
    "    Xtr_raw, ytr_raw, Xval_raw, yval_raw = split_windows_safe(X_raw, y_raw, val_frac=VAL_FRAC, min_train=1, min_val=1)\n",
    "    n_tr, n_val = len(Xtr_raw), len(Xval_raw)\n",
    "    if n_tr == 0:\n",
    "        raise ValueError(\"No training windows created. Reduce LOOKBACK/HORIZON or add data.\")\n",
    "    has_val = n_val > 0\n",
    "\n",
    "\n",
    "    x_scaler = StandardScaler().fit(Xtr_raw.reshape(-1, m_in))\n",
    "    Xtr  = (Xtr_raw - x_scaler.mean_) / x_scaler.scale_\n",
    "    Xval = (Xval_raw - x_scaler.mean_) / x_scaler.scale_ if has_val else Xval_raw\n",
    "\n",
    "    if USE_Y_SCALING:\n",
    "        ytr_mat  = ytr_raw.reshape(-1, H, m_out).reshape(-1, m_out)\n",
    "        from sklearn.preprocessing import StandardScaler as _SS\n",
    "        y_scaler = _SS().fit(ytr_mat)\n",
    "        ytr = ((ytr_mat - y_scaler.mean_) / y_scaler.scale_).reshape(-1, H*m_out)\n",
    "        if has_val:\n",
    "            yval_mat = yval_raw.reshape(-1, H, m_out).reshape(-1, m_out)\n",
    "            yval = ((yval_mat - y_scaler.mean_) / y_scaler.scale_).reshape(-1, H*m_out)\n",
    "        else:\n",
    "            yval = yval_raw\n",
    "    else:\n",
    "        y_scaler = None\n",
    "        ytr, yval = ytr_raw, yval_raw\n",
    "\n",
    "    def build_callbacks(has_val):\n",
    "        if has_val:\n",
    "            return [\n",
    "                callbacks.EarlyStopping(monitor=\"val_loss\", patience=PATIENCE, restore_best_weights=True),\n",
    "                callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=max(8, PATIENCE//2), min_lr=1e-5),\n",
    "            ]\n",
    "        else:\n",
    "            return [\n",
    "                callbacks.EarlyStopping(monitor=\"loss\", patience=PATIENCE, restore_best_weights=True),\n",
    "                callbacks.ReduceLROnPlateau(monitor=\"loss\", factor=0.5, patience=max(8, PATIENCE//2), min_lr=1e-5),\n",
    "            ]\n",
    "\n",
    "    eff_batch = max(1, min(BATCH, len(Xtr)))\n",
    "    results, best = [], {\"kind\": None, \"val_smape\": 1e9, \"model\": None}\n",
    "    for kind in [\"rnn\",\"lstm\",\"cnn\",\"cnn_rnn\",\"cnn_lstm\"]:\n",
    "        mdl = MODEL_BUILDERS[kind](L, m_in, H)\n",
    "        cbs = build_callbacks(has_val)\n",
    "        if has_val:\n",
    "            mdl.fit(Xtr, ytr, validation_data=(Xval, yval), epochs=EPOCHS, batch_size=eff_batch, verbose=0, callbacks=cbs)\n",
    "            yhat_val = mdl.predict(Xval, verbose=0)\n",
    "            if y_scaler is not None:\n",
    "                yhat_val_mat = yhat_val.reshape(-1, H, m_out).reshape(-1, m_out)\n",
    "                yval_mat     = yval.reshape(-1, H, m_out).reshape(-1, m_out)\n",
    "                yhat_eval = yhat_val_mat * y_scaler.scale_ + y_scaler.mean_\n",
    "                yval_eval = yval_mat     * y_scaler.scale_ + y_scaler.mean_\n",
    "            else:\n",
    "                yhat_eval, yval_eval = yhat_val, yval\n",
    "        else:\n",
    "            mdl.fit(Xtr, ytr, epochs=EPOCHS, batch_size=eff_batch, verbose=0, callbacks=cbs)\n",
    "            yhat_tr = mdl.predict(Xtr, verbose=0)\n",
    "            if y_scaler is not None:\n",
    "                yhat_tr_mat = yhat_tr.reshape(-1, H, m_out).reshape(-1, m_out)\n",
    "                ytr_mat2    = ytr.reshape(-1, H, m_out).reshape(-1, m_out)\n",
    "                yhat_eval = yhat_tr_mat * y_scaler.scale_ + y_scaler.mean_\n",
    "                yval_eval = ytr_mat2    * y_scaler.scale_ + y_scaler.mean_\n",
    "            else:\n",
    "                yhat_eval, yval_eval = yhat_tr, ytr\n",
    "\n",
    "        sc = {\n",
    "            \"kind\": kind,\n",
    "            \"val_rmse\": float(np.sqrt(np.mean((yval_eval - yhat_eval)**2))),\n",
    "            \"val_smape\": float(np.mean(\n",
    "                np.abs(yval_eval - yhat_eval) /\n",
    "                np.maximum((np.abs(yval_eval)+np.abs(yhat_eval))/2.0, 1e-8)\n",
    "            )*100.0)\n",
    "        }\n",
    "        results.append(sc)\n",
    "        if sc[\"val_smape\"] < best[\"val_smape\"]:\n",
    "            best.update({\"kind\": kind, \"val_smape\": sc[\"val_smape\"], \"model\": mdl})\n",
    "\n",
    "    return best[\"model\"], best[\"kind\"], x_scaler, y_scaler, L, m_in, H, pd.DataFrame(results).sort_values(\"val_smape\")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Optional: short fine-tune on selected scenarios\n",
    "# -------------------------------------------------------\n",
    "def finetune_on_scenarios(model, df, scenarios, lookback, horizon,\n",
    "                          x_scaler, y_scaler, epochs=3, batch_size=32,\n",
    "                          augment_repeat=1, min_train_windows=2):\n",
    "    df_sub = make_df_use(df)\n",
    "    df_sub = df_sub[df_sub[\"scenario\"].isin(scenarios)].copy()\n",
    "    rep = int(augment_repeat if augment_repeat is not None else 1)\n",
    "    while rep >= 1:\n",
    "        try:\n",
    "            X_raw, y_raw = make_windows_from_tidy(df_sub, lookback, horizon, augment_repeat=rep)\n",
    "            break\n",
    "        except ValueError as e:\n",
    "            if \"No/too few windows\" in str(e) and rep > 1:\n",
    "                print(f\"[Fine-tune] Not enough windows with repeat={rep}. Trying repeat={rep-1} ...\")\n",
    "                rep -= 1\n",
    "                continue\n",
    "            print(f\"[Fine-tune] Skipping: {e}\")\n",
    "            return model\n",
    "    if X_raw.shape[0] < max(1, min_train_windows):\n",
    "        print(f\"[Fine-tune] Skipping: only {X_raw.shape[0]} window(s) available (need ≥{min_train_windows}).\")\n",
    "        return model\n",
    "    m_out = len(PRODUCTS); H = y_raw.shape[1] // m_out\n",
    "    X = (X_raw - x_scaler.mean_) / x_scaler.scale_\n",
    "    if y_scaler is not None:\n",
    "        y_mat = y_raw.reshape(-1, H, m_out).reshape(-1, m_out)\n",
    "        y = ((y_mat - y_scaler.mean_) / y_scaler.scale_).reshape(-1, H*m_out)\n",
    "    else:\n",
    "        y = y_raw\n",
    "    eff_bs = max(1, min(batch_size, X.shape[0]))\n",
    "    model.fit(X, y, epochs=epochs, batch_size=eff_bs, verbose=0)\n",
    "    print(f\"[Fine-tune] Done on scenarios={scenarios} with repeat={rep}, windows={X.shape[0]}, epochs={epochs}\")\n",
    "    return model\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Forecast with the SAME feature pipeline used in training\n",
    "# -------------------------------------------------------\n",
    "def forecast_baseline_for_scenario(df, model, x_scaler, y_scaler, scen,\n",
    "                                   horizon=HORIZON, lookback=LOOKBACK):\n",
    "    \"\"\"\n",
    "    Rebuild the last window with make_windows_from_tidy for this scenario,\n",
    "    scale with x_scaler, predict H×m_out, invert y_scaler, return DataFrame.\n",
    "\n",
    "    NOTE: Horizon is effectively limited by the model's trained H\n",
    "          (H_trained = output_dim / len(PRODUCTS)).\n",
    "    \"\"\"\n",
    "    df_use = make_df_use(df)\n",
    "    df_s   = df_use[df_use[\"scenario\"] == scen].copy()\n",
    "\n",
    "    # Build windows only to grab the *last* input window; horizon=1 is enough for that\n",
    "    X_raw, _ = make_windows_from_tidy(df_s, lookback=lookback, horizon=1, augment_repeat=None)\n",
    "    if X_raw.ndim != 3 or X_raw.shape[0] == 0:\n",
    "        raise ValueError(f\"make_windows_from_tidy produced no window for scenario={scen}\")\n",
    "\n",
    "    X_last = X_raw[-1]  # (L, m_in)\n",
    "\n",
    "    # Sanity: ensure m_in matches scaler\n",
    "    assert X_last.shape[1] == len(x_scaler.mean_), \\\n",
    "        f\"m_in mismatch: {X_last.shape[1]} vs {len(x_scaler.mean_)}\"\n",
    "\n",
    "    # Scale features per-column\n",
    "    X_in = (X_last - x_scaler.mean_[None, :]) / x_scaler.scale_[None, :]\n",
    "    X_in = X_in[None, ...]  # (1, L, m_in)\n",
    "\n",
    "    # Raw model output: flat vector of length H_trained * m_out\n",
    "    yhat_flat = model.predict(X_in, verbose=0).reshape(-1)\n",
    "    m_out = len(PRODUCTS)\n",
    "    H_trained = yhat_flat.size // m_out\n",
    "\n",
    "    if yhat_flat.size % m_out != 0:\n",
    "        raise ValueError(f\"Model output size {yhat_flat.size} is not divisible by m_out={m_out}.\")\n",
    "\n",
    "    # Use the *minimum* of requested horizon and model's trained horizon\n",
    "    H_eff = min(horizon, H_trained)\n",
    "\n",
    "    yhat_scaled = yhat_flat.reshape(H_trained, m_out)[:H_eff, :]  # (H_eff, m_out)\n",
    "\n",
    "    if y_scaler is not None:\n",
    "        yhat = (\n",
    "            yhat_scaled * y_scaler.scale_.reshape(1, m_out) +\n",
    "            y_scaler.mean_.reshape(1, m_out)\n",
    "        )\n",
    "    else:\n",
    "        yhat = yhat_scaled\n",
    "\n",
    "    out = pd.DataFrame(yhat, columns=PRODUCTS)\n",
    "    out.insert(0, \"horizon\", np.arange(1, H_eff + 1))\n",
    "    out.insert(0, \"scenario\", scen)\n",
    "    return out\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Tariff / elasticity post-processing (optional)\n",
    "# -------------------------------------------------------\n",
    "def apply_tariff_elasticity(fore_df, tariff_path, elasticity, pass_through, china_share):\n",
    "    adj = fore_df.copy()\n",
    "    for idx, row in adj.iterrows():\n",
    "        h = int(row[\"horizon\"])\n",
    "        tau = float(tariff_path.get(h, 0.0))\n",
    "        for prod in PRODUCTS:\n",
    "            eps = elasticity[prod]\n",
    "            rho = pass_through[prod]\n",
    "            sCN = china_share[prod]\n",
    "            factor = 1.0 + eps * rho * tau * sCN\n",
    "            adj.at[idx, prod] = max(0.0, row[prod] * factor)\n",
    "    return adj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3460ef60-0c45-4598-9f15-055d5d117b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRODUCTS (normalized): ['MacBook', 'iPad', 'iPhone']\n",
      "Best DL model: rnn\n",
      "       kind      val_rmse  val_smape\n",
      "0       rnn   4032.295493   4.199388\n",
      "4  cnn_lstm  10231.835729   7.815067\n",
      "3   cnn_rnn   9887.872111   8.101153\n",
      "1      lstm  11090.118658   8.468427\n",
      "2       cnn  11214.696970   8.670750\n",
      "[Fine-tune] Done on scenarios=['Holiday_Q4_Surge', 'Launch_Spike_Sep'] with repeat=1, windows=74, epochs=3\n",
      "             scenario  horizon       MacBook          iPad         iPhone\n",
      "0   Baseline_Seasonal        1  39437.919757  36310.000013  133745.051780\n",
      "1   Baseline_Seasonal        2  33363.900929  33872.749781  107601.750306\n",
      "2   Baseline_Seasonal        3  32740.717801  34099.994696  106623.826766\n",
      "3   Baseline_Seasonal        4  35027.286706  35515.332453  114329.740940\n",
      "4   Baseline_Seasonal        5  36550.484220  36571.889321  117162.717638\n",
      "5   Baseline_Seasonal        6  37951.259113  39211.896026  118704.736349\n",
      "6   Baseline_Seasonal        7  39079.707425  39328.012998  120476.899906\n",
      "7   Baseline_Seasonal        8  37163.902726  35811.509965  109417.235908\n",
      "8   Baseline_Seasonal        9  40136.408233  41310.390587  111781.457294\n",
      "9   Baseline_Seasonal       10  40562.542620  39766.867040  121245.055142\n",
      "10  Baseline_Seasonal       11  37567.241013  40084.440777  129178.005865\n",
      "11  Baseline_Seasonal       12  38249.372207  40193.585753  137781.010790\n",
      "             scenario  horizon       MacBook          iPad         iPhone\n",
      "0   Baseline_Seasonal        1  38732.966942  35874.280013  130374.676476\n",
      "1   Baseline_Seasonal        2  32767.521199  33466.276784  104890.186198\n",
      "2   Baseline_Seasonal        3  31570.237140  33281.594823  101249.985897\n",
      "3   Baseline_Seasonal        4  35027.286706  35515.332453  114329.740940\n",
      "4   Baseline_Seasonal        5  36550.484220  36571.889321  117162.717638\n",
      "5   Baseline_Seasonal        6  37951.259113  39211.896026  118704.736349\n",
      "6   Baseline_Seasonal        7  39079.707425  39328.012998  120476.899906\n",
      "7   Baseline_Seasonal        8  37163.902726  35811.509965  109417.235908\n",
      "8   Baseline_Seasonal        9  40136.408233  41310.390587  111781.457294\n",
      "9   Baseline_Seasonal       10  40562.542620  39766.867040  121245.055142\n",
      "10  Baseline_Seasonal       11  37567.241013  40084.440777  129178.005865\n",
      "11  Baseline_Seasonal       12  38249.372207  40193.585753  137781.010790\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------\n",
    "# Example usage\n",
    "# -------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    df_raw = pd.read_csv(r\"C:\\Users\\marieska\\Downloads\\monthly_demand_regions_weighted_2020_2024_growth_corrected_Adjusted.csv\")\n",
    "    df_use = make_df_use(df_raw)\n",
    "    PRODUCTS = sorted({_norm_prod(p) for p in df_use[\"product_name\"].unique()})\n",
    "    print(\"PRODUCTS (normalized):\", PRODUCTS)\n",
    "\n",
    "    # 2) Train model zoo and pick best on validation sMAPE\n",
    "    model, best_kind, x_sc, y_sc, L, m_in, H, table = run_model_zoo(df_use, LOOKBACK, HORIZON, augment_repeat=1)\n",
    "    print(\"Best DL model:\", best_kind)\n",
    "    print(table)\n",
    "\n",
    "    # 3) (Optional) Fine-tune on tougher scenarios (robust to low-window scenarios)\n",
    "    hard = [\"Holiday_Q4_Surge\", \"Launch_Spike_Sep\"]\n",
    "    model = finetune_on_scenarios(\n",
    "        model, df_use, hard,\n",
    "        lookback=min(LOOKBACK, 12),\n",
    "        horizon=HORIZON,\n",
    "        x_scaler=x_sc, y_scaler=y_sc,\n",
    "        epochs=3, batch_size=32,\n",
    "        augment_repeat=1\n",
    "    )\n",
    "\n",
    "    # 4) Forecast for baseline\n",
    "    SCEN = \"Baseline_Seasonal\"\n",
    "    fc = forecast_baseline_for_scenario(df_use, model, x_sc, y_sc, SCEN, horizon=HORIZON, lookback=LOOKBACK)\n",
    "    print(fc.head(HORIZON))\n",
    "\n",
    "    # 5) (Optional) tariff adjustment\n",
    "    tariff_path = {1: 0.05, 2: 0.05, 3: 0.10}  # example path\n",
    "    ELASTICITY   = {\"iPhone\": -1.2, \"iPad\": -0.8, \"MacBook\": -1.0}\n",
    "    PASS_THROUGH = {\"iPhone\": 0.7,  \"iPad\": 0.6,  \"MacBook\": 0.65}\n",
    "    CHINA_SHARE  = {\"iPhone\": 0.6,  \"iPad\": 0.5,  \"MacBook\": 0.55}\n",
    "    fc_tariff = apply_tariff_elasticity(fc, tariff_path, ELASTICITY, PASS_THROUGH, CHINA_SHARE)\n",
    "    print(fc_tariff.head(HORIZON))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5c62b253-0253-423c-9369-f0ffa96720ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>period</th>\n",
       "      <th>product_name</th>\n",
       "      <th>scenario</th>\n",
       "      <th>demand_units</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>MacBook</td>\n",
       "      <td>Back_to_School</td>\n",
       "      <td>29434.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>MacBook</td>\n",
       "      <td>Baseline_Seasonal</td>\n",
       "      <td>31116.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>MacBook</td>\n",
       "      <td>Combined_Stress</td>\n",
       "      <td>31793.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>MacBook</td>\n",
       "      <td>Holiday_Q4_Surge</td>\n",
       "      <td>30143.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>MacBook</td>\n",
       "      <td>Launch_Spike_Sep</td>\n",
       "      <td>30322.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year  period product_name           scenario  demand_units\n",
       "0  2020       1      MacBook     Back_to_School       29434.0\n",
       "1  2020       1      MacBook  Baseline_Seasonal       31116.0\n",
       "2  2020       1      MacBook    Combined_Stress       31793.0\n",
       "3  2020       1      MacBook   Holiday_Q4_Surge       30143.0\n",
       "4  2020       1      MacBook   Launch_Spike_Sep       30322.0"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_use.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2e3ef2e7-3281-4636-9ab3-7b9dbdccf142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last-12 medians: product_name\n",
      "iPhone     117960.629750\n",
      "iPad        38240.374160\n",
      "MacBook     36386.311885\n",
      "dtype: float64\n",
      "DL h=1–12 median: iPhone     117933.726993\n",
      "iPad        37891.892673\n",
      "MacBook     37759.250063\n",
      "dtype: float64\n",
      "DL vs last-H sMAPE (proxy backtest): 5.566627335522847\n",
      "Naive vs last-H sMAPE: 71.90267054766349\n"
     ]
    }
   ],
   "source": [
    "# 1) Compare medians to last-year medians (scale sanity)\n",
    "pv = panel_for_scenario_full(df_use, \"Baseline_Seasonal\")\n",
    "print(\"last-12 medians:\", pv.tail(12)[[\"iPhone\",\"iPad\",\"MacBook\"]].median())\n",
    "print(\"DL h=1–12 median:\", fc[[\"iPhone\",\"iPad\",\"MacBook\"]].median())\n",
    "\n",
    "# 2) Compare DL to naive same-month-last-year on last H months\n",
    "y_true_lastH = pv.values.astype(\"float64\")[-HORIZON:, :]\n",
    "yhat_naive   = naive_same_month_last_year(df_use, \"Baseline_Seasonal\", HORIZON, LOOKBACK)\n",
    "print(\"DL vs last-H sMAPE (proxy backtest):\", smape_vec(y_true_lastH, fc[[\"iPhone\",\"iPad\",\"MacBook\"]].to_numpy()))\n",
    "print(\"Naive vs last-H sMAPE:\", smape_vec(y_true_lastH, yhat_naive))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "642bc26c-e328-4f6a-a4eb-1da39d15fe68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# EVALUATION: last-fold comparison\n",
    "# ================================\n",
    "# --- metrics (epsilon-safe) ---\n",
    "\n",
    "def _panel_for_scenario_rect(df, scen, products):\n",
    "    d = make_df_use(df)\n",
    "    d = normalize_time_strict(d)\n",
    "    g = d[d[\"scenario\"] == scen].copy()\n",
    "    pv = (g.pivot_table(index=\"t_month\", columns=\"product_name\", values=\"demand_units\", aggfunc=\"first\")\n",
    "            .reindex(columns=products))\n",
    "    return d, pv\n",
    "\n",
    "def _cut_df_to_t_end(df_use_norm, scen, t_end):\n",
    "    \"\"\"Return a df with only rows of the given scenario where t_month < t_end (drop helper col).\"\"\"\n",
    "    cols_keep = [c for c in df_use_norm.columns if c != \"t_month\"]\n",
    "    mask = (df_use_norm[\"scenario\"]==scen) & (df_use_norm[\"t_month\"] < t_end)\n",
    "    return df_use_norm.loc[mask, cols_keep].copy()\n",
    "\n",
    "\n",
    "def _to_array(df_fc, products=None):\n",
    "    \"\"\"H x m array from a forecast DataFrame with columns ['scenario','horizon', <products>].\"\"\"\n",
    "    if products is None:\n",
    "        products = [c for c in df_fc.columns if c not in (\"scenario\",\"horizon\")]\n",
    "    arr = df_fc.sort_values(\"horizon\")[products].to_numpy(dtype=\"float64\")\n",
    "    return arr\n",
    "\n",
    "# def smape_eps(y_true, y_pred, eps=1e-8):\n",
    "#     \"\"\"sMAPE (%) with epsilon to avoid 0/0 on small values.\"\"\"\n",
    "#     yt = np.asarray(y_true, dtype=\"float64\").ravel()\n",
    "#     yp = np.asarray(y_pred, dtype=\"float64\").ravel()\n",
    "#     denom = (np.abs(yt) + np.abs(yp)) / 2.0\n",
    "#     return float(np.mean(np.abs(yt - yp) / np.maximum(denom, eps)) * 100.0)\n",
    "\n",
    "\n",
    "# ---------- Metrics (drop-in patch) ----------\n",
    "def smape_eps(y_true, y_pred, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Symmetric MAPE with numerical safeguard.\n",
    "    Returns percent (0..200).\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=\"float64\")\n",
    "    y_pred = np.asarray(y_pred, dtype=\"float64\")\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred)) / 2.0\n",
    "    return float(np.mean(np.abs(y_true - y_pred) / np.maximum(denom, eps)) * 100.0)\n",
    "\n",
    "def wape_eps(y_true, y_pred, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Weighted absolute percentage error with safeguard.\n",
    "    Returns percent (0..∞).\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=\"float64\")\n",
    "    y_pred = np.asarray(y_pred, dtype=\"float64\")\n",
    "    num = np.sum(np.abs(y_true - y_pred))\n",
    "    den = np.sum(np.abs(y_true))\n",
    "    return float((num / max(den, eps)) * 100.0)\n",
    "\n",
    "def rmse_vec(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=\"float64\")\n",
    "    y_pred = np.asarray(y_pred, dtype=\"float64\")\n",
    "    return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
    "\n",
    "def rmsle_log1p(y_true, y_pred, eps=1e-12):\n",
    "    y_true = np.asarray(y_true, dtype=\"float64\")\n",
    "    y_pred = np.asarray(y_pred, dtype=\"float64\")\n",
    "    return float(np.sqrt(np.mean((np.log1p(np.maximum(y_true, 0)+eps) -\n",
    "                                  np.log1p(np.maximum(y_pred, 0)+eps)) ** 2)))\n",
    "\n",
    "def wape_eps(y_true, y_pred, eps=1e-8):\n",
    "    \"\"\"WAPE (%) with epsilon on denominator.\"\"\"\n",
    "    yt = np.asarray(y_true, dtype=\"float64\").ravel()\n",
    "    yp = np.asarray(y_pred, dtype=\"float64\").ravel()\n",
    "    return float(np.sum(np.abs(yt - yp)) / np.maximum(np.sum(np.abs(yt)), eps) * 100.0)\n",
    "\n",
    "def rmse_vec(y_true, y_pred):\n",
    "    yt = np.asarray(y_true, dtype=\"float64\").ravel()\n",
    "    yp = np.asarray(y_pred, dtype=\"float64\").ravel()\n",
    "    return float(np.sqrt(np.mean((yt - yp)**2)))\n",
    "\n",
    "def rmsle_log1p(y_true, y_pred, clip_min=0.0):\n",
    "    yt = np.asarray(y_true, dtype=\"float64\").ravel()\n",
    "    yp = np.asarray(y_pred, dtype=\"float64\").ravel()\n",
    "    yp = np.maximum(yp, clip_min)  # avoid log of negatives\n",
    "    return float(np.sqrt(np.mean((np.log1p(yt) - np.log1p(yp))**2)))\n",
    "\n",
    "####------------------------------------------------------\n",
    "# ---------- DL forecast helper that matches the training pipeline ----------\n",
    "def _make_extras_for_last_L(df_norm, scen, lookback, products, expected_m_in):\n",
    "    \"\"\"\n",
    "    Build the SAME extras used during DL training:\n",
    "      - 11 month one-hots m_2..m_12 (no m_1)\n",
    "      - normalized trend t_norm in [0,1]\n",
    "      - Fourier seasonality (sin/cos) based on month\n",
    "    Returns an (L, n_extras) float32 array.\n",
    "    \"\"\"\n",
    "    # We assume df_norm already has: scenario, Year, period, t_month, product_name, demand_units\n",
    "    g = (df_norm[df_norm[\"scenario\"] == scen]\n",
    "         .drop_duplicates(subset=[\"t_month\"])\n",
    "         .sort_values(\"t_month\")[[\"t_month\", \"Year\", \"period\"]])\n",
    "\n",
    "    if len(g) < lookback:\n",
    "        raise ValueError(f\"Not enough rows for lookback={lookback}. Have {len(g)}.\")\n",
    "\n",
    "    last = g.iloc[-lookback:].copy()\n",
    "    # Month one-hots m_2..m_12 (no m_1)\n",
    "    for k in range(2, 13):\n",
    "        last[f\"m_{k}\"] = (last[\"period\"].astype(int) == k).astype(float)\n",
    "\n",
    "    # Trend normalization on t_month (within the entire scenario time span)\n",
    "    t_min, t_max = g[\"t_month\"].min(), g[\"t_month\"].max()\n",
    "    denom = max(1.0, float(t_max - t_min))\n",
    "    last[\"t_norm\"] = (last[\"t_month\"] - t_min) / denom\n",
    "\n",
    "    # Fourier seasonality from month\n",
    "    m = last[\"period\"].astype(int).to_numpy()\n",
    "    last[\"month_sin\"] = np.sin(2*np.pi*(m/12.0))\n",
    "    last[\"month_cos\"] = np.cos(2*np.pi*(m/12.0))\n",
    "\n",
    "    # Final extras in the same canonical order we’ll assume was used in training:\n",
    "    extra_cols = [f\"m_{k}\" for k in range(2,13)] + [\"t_norm\", \"month_sin\", \"month_cos\"]\n",
    "    X_extras = last[extra_cols].to_numpy(dtype=\"float32\")\n",
    "\n",
    "    # Sanity: check width: 11 + 1 + 2 = 14\n",
    "    # Products are 3 → total expected 17\n",
    "    total_expected = len(products) + X_extras.shape[1]\n",
    "    if total_expected != expected_m_in:\n",
    "        raise AssertionError(\n",
    "            f\"Extras width mismatch. Products={len(products)}, extras={X_extras.shape[1]} \"\n",
    "            f\"→ total {total_expected} but model expects m_in={expected_m_in}. \"\n",
    "            f\"Adjust extras to match training.\"\n",
    "        )\n",
    "    return X_extras\n",
    "\n",
    "\n",
    "def forecast_dl_for_eval(model, x_scaler, y_scaler, df_use,\n",
    "                         scen, products, lookback, horizon,\n",
    "                         m_in_trained, m_out_trained):\n",
    "    \"\"\"\n",
    "    Build an input of shape (1, L, m_in_trained) that matches training:\n",
    "    [PRODUCTS (3)] + [m_2..m_12 (11)] + [t_norm (1)] + [month_sin, month_cos (2)] = 17\n",
    "    \"\"\"\n",
    "    # 1) Continuous timeline + panel (T × m_out)\n",
    "    df_norm = normalize_time_strict(df_use)\n",
    "    pv = panel_for_scenario_matrix_safe(\n",
    "        df_norm, scen,\n",
    "        prod_col=\"product_name\", dem_col=\"demand_units\",\n",
    "        time_col=\"t_month\", scen_col=\"scenario\",\n",
    "        products=products, impute=\"drop\"\n",
    "    )  # index=t_month, columns=PRODUCTS\n",
    "\n",
    "    # 2) Last L window of the 3 product series\n",
    "    last_L_products = pv.values.astype(\"float32\")[-lookback:, :]   # (L, 3)\n",
    "\n",
    "    # 3) Recreate the extras exactly as in training\n",
    "    X_extras = _make_extras_for_last_L(df_norm, scen, lookback, products, expected_m_in=m_in_trained)  # (L, 14)\n",
    "\n",
    "    # 4) Concatenate to width 17\n",
    "    last_L_full = np.concatenate([last_L_products, X_extras], axis=1)  # (L, 17)\n",
    "    assert last_L_full.shape[1] == m_in_trained\n",
    "\n",
    "    # 5) Scale with TRAIN stats, predict, invert y scale\n",
    "    X_in = (last_L_full - x_scaler.mean_) / x_scaler.scale_\n",
    "    X_in = X_in[None, ...]  # (1, L, 17)\n",
    "\n",
    "    yhat_scaled = model.predict(X_in, verbose=0).reshape(horizon, m_out_trained)\n",
    "    if y_scaler is not None:\n",
    "        yhat = yhat_scaled * y_scaler.scale_.reshape(1, m_out_trained) + y_scaler.mean_.reshape(1, m_out_trained)\n",
    "    else:\n",
    "        yhat = yhat_scaled\n",
    "    yhat = np.clip(yhat, 0.0, None)\n",
    "    return yhat\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------\n",
    "\n",
    "  \n",
    "def evaluate_last_fold(\n",
    "    df_raw,                                    # original dataframe\n",
    "    scen=\"Baseline_Seasonal\",\n",
    "    model_name_map=None,                       # dict: {name: forecast_df} optional (XGB/Prophet/SARIMAX/Ensembles)\n",
    "    dl_tuple=None,                             # (model, x_scaler, y_scaler) for your deep model\n",
    "    products=None,\n",
    "    lookback=None,\n",
    "    horizon=None,\n",
    "    save_prefix=\"eval_lastfold\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Cuts the scenario at the last feasible fold (t_end), forecasts the next H months per model,\n",
    "    compares against ground truth, prints & saves tables.\n",
    "    \"\"\"\n",
    "    products = products or PRODUCTS\n",
    "    lookback = lookback or LOOKBACK\n",
    "    horizon  = horizon  or HORIZON\n",
    "    model_name_map = dict(model_name_map or {})  # copy\n",
    "\n",
    "    # 1) set up normalized timeline & panel\n",
    "    df_norm, panel = _panel_for_scenario_rect(df_raw, scen, products)\n",
    "    mat_full = panel.to_numpy(float)\n",
    "    T = mat_full.shape[0]\n",
    "    if T < lookback + horizon + 1:\n",
    "        raise ValueError(f\"Not enough history T={T} for lookback={lookback}, horizon={horizon}\")\n",
    "\n",
    "    t_end = T - horizon  # last fold anchor\n",
    "    y_true = mat_full[t_end : t_end + horizon, :]  # H × m\n",
    "\n",
    "    print(f\"\\nEvaluating against last-fold (t_end={t_end}) ground truth for scenario '{scen}'.\\n\")\n",
    "    rows_all, rows_pp = [], []\n",
    "        # ---- Deep Learning model scoring ----\n",
    "    if dl_tuple is not None:\n",
    "        mdl, xs, ys, Ldl, mindl, Hdl = dl_tuple\n",
    "        assert Hdl == horizon, f\"DL horizon {Hdl} != eval horizon {horizon}\"\n",
    "        y_pred_dl = forecast_dl_for_eval(\n",
    "            mdl, xs, ys, df_raw, scen, products,\n",
    "            lookback=lookback, horizon=horizon,\n",
    "            m_in_trained=mindl, m_out_trained=len(products)\n",
    "        )\n",
    "    \n",
    "        rows_all.append({\n",
    "            \"model\": \"DL_Best\",\n",
    "            \"N points\": int(y_pred_dl.size),\n",
    "            \"sMAPE_eps (%)\": smape_eps(y_true, y_pred_dl),\n",
    "            \"WAPE_eps (%)\":  wape_eps(y_true, y_pred_dl),\n",
    "            \"RMSE\":          rmse_vec(y_true, y_pred_dl),\n",
    "            \"RMSLE (log1p)\": rmsle_log1p(y_true, y_pred_dl),\n",
    "        })\n",
    "    \n",
    "        for j, prod in enumerate(products):\n",
    "            rows_pp.append({\n",
    "                \"product_name\": prod, \"model\": \"DL_Best\",\n",
    "                \"N points\": int(horizon),\n",
    "                \"sMAPE_eps (%)\": smape_eps(y_true[:, j], y_pred_dl[:, j]),\n",
    "                \"WAPE_eps (%)\":  wape_eps(y_true[:, j], y_pred_dl[:, j]),\n",
    "                \"RMSE\":          rmse_vec(y_true[:, j], y_pred_dl[:, j]),\n",
    "                \"RMSLE (log1p)\": rmsle_log1p(y_true[:, j], y_pred_dl[:, j]),\n",
    "            })\n",
    "\n",
    "\n",
    "    \n",
    "    overall = pd.DataFrame(rows_all).sort_values(\"sMAPE_eps (%)\").reset_index(drop=True)\n",
    "    perprod = pd.DataFrame(rows_pp).sort_values([\"product_name\",\"sMAPE_eps (%)\"]).reset_index(drop=True)\n",
    "\n",
    "    # 5) build a sMAPE matrix (rows=product, cols=model)\n",
    "    sm_mat = perprod.pivot(index=\"product_name\", columns=\"model\", values=\"sMAPE_eps (%)\")\n",
    "    sm_mat = sm_mat.reindex(index=products)  # keep product order\n",
    "\n",
    "    # 6) print nicely\n",
    "    print(\"=== Overall (incl. ensembles/DL) ===\")\n",
    "    print(overall.to_string(index=False))\n",
    "    print(\"\\n=== Per-product (incl. ensembles/DL) ===\")\n",
    "    print(perprod.to_string(index=False))\n",
    "    print(\"\\n=== sMAPE_eps matrix (rows=product, cols=model) ===\")\n",
    "    print(sm_mat.round(2).to_string())\n",
    "\n",
    "    # 7) save CSVs for Word\n",
    "    overall.to_csv(f\"{save_prefix}_overall.csv\", index=False)\n",
    "    perprod.to_csv(f\"{save_prefix}_per_product.csv\", index=False)\n",
    "    sm_mat.to_csv(f\"{save_prefix}_smape_matrix.csv\")\n",
    "\n",
    "    # 8) also return them in case you want to keep in memory\n",
    "    return overall, perprod, sm_mat\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5dbf52-e5f5-4bc4-9903-08baea828544",
   "metadata": {},
   "source": [
    "# PART2: XGBoost, SARIMA,..... (Machine Learning Models) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0f8ce404-c356-4e62-87c4-7f442967eda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BLOCK 1:CELL 1 — Imports & global config #\n",
    "# Assumptions & tiny helpers\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np, pandas as pd\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from prophet import Prophet\n",
    "\n",
    "# --- CONFIG---\n",
    "LOOKBACK = 12    # L: timesteps to look back (e.g., 12 months)\n",
    "LOOKBACK_PLAN = 12\n",
    "HORIZON  = 3     # H: steps ahead to predict (e.g., next 6 months)\n",
    "HORIZON_PLAN=3\n",
    "VAL_FRAC = 0.20  # fraction of data for validation (chronological)\n",
    "# TEST_FRAC=0.15 # fraction for test\n",
    "BATCH    = 32  \n",
    "EPOCHS   = 300\n",
    "PATIENCE = 30\n",
    "LR       = 1e-3\n",
    "SEED     = 42\n",
    "\n",
    "PRODUCTS = [\"iPhone\",\"iPad\",\"MacBook\"]\n",
    "\n",
    "ELASTICITY   = {\"iPhone\": -1.1, \"iPad\": -0.9, \"MacBook\": -1.3}\n",
    "PASS_THROUGH = {\"iPhone\": 0.35, \"iPad\": 0.30, \"MacBook\": 0.40}\n",
    "CHINA_SHARE  = {\"iPhone\": 0.65, \"iPad\": 0.55, \"MacBook\": 0.45}\n",
    "TARIFF_PATH  = {h: 0.25 for h in range(1, HORIZON+1)}\n",
    "\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\marieska\\Downloads\\monthly_demand_regions_weighted_2020_2024_growth_corrected_Adjusted.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7ffede17-a4e3-43c5-b92f-7d3a3f8809bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL 2 — Generic helpers (single source of truth)\n",
    "\n",
    "def _pick_col(df, candidates, required=True):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    if required:\n",
    "        raise KeyError(f\"None of the columns {candidates} found. Available: {list(df.columns)}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "##----------------------------------------------------------------------------------------------------------    \n",
    "\n",
    "def ensure_time_columns_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Standardize column names & types that the rest of the pipeline expects.\n",
    "    Guarantees the presence of: scenario, Year, period, product_name, demand_units\n",
    "    If a 'ds' (timestamp) exists, derives Year/period from it.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    year_col = _pick_col(df, [\"Year\", \"year\"], required=False)\n",
    "    per_col  = _pick_col(df, [\"period\", \"month\", \"Month\"], required=False)\n",
    "    ds_col   = _pick_col(df, [\"ds\", \"date\", \"Date\"], required=False)\n",
    "\n",
    "    if ds_col is not None and (year_col is None or per_col is None):\n",
    "        ds = pd.to_datetime(df[ds_col])\n",
    "        df[\"Year\"]   = ds.dt.year.astype(int)\n",
    "        df[\"period\"] = ds.dt.month.astype(int)\n",
    "        df[\"ds\"]     = ds.dt.to_period(\"M\").dt.to_timestamp(how=\"start\")\n",
    "    else:\n",
    "        if year_col is None or per_col is None:\n",
    "            raise KeyError(\"Need either 'ds' or both (Year/year) and (period/month/Month).\")\n",
    "\n",
    "        df[\"Year\"]   = pd.to_numeric(df[year_col], errors=\"coerce\").astype(\"Int64\")\n",
    "        df[\"period\"] = pd.to_numeric(df[per_col],  errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "        # Validate 1..12\n",
    "        if not df[\"period\"].between(1, 12).fillna(False).all():\n",
    "            bad = df.loc[~df[\"period\"].between(1,12).fillna(False), [\"Year\",\"period\"]]\n",
    "            raise ValueError(f\"'period' must be 1..12. Bad rows:\\n{bad.head()}\")\n",
    "\n",
    "        df[\"ds\"] = pd.to_datetime(dict(\n",
    "            year=df[\"Year\"].astype(int),\n",
    "            month=df[\"period\"].astype(int),\n",
    "            day=1\n",
    "        ))\n",
    "\n",
    "    # Final types + sort\n",
    "    df[\"Year\"]   = df[\"Year\"].astype(int)\n",
    "    df[\"period\"] = df[\"period\"].astype(int)\n",
    "    df = df.sort_values([\"Year\", \"period\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "##----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def ensure_time_index(df):\n",
    "    \"\"\"\n",
    "    Build a true month timeline per scenario and return:\n",
    "    (DataFrame_with_t_month, scen_col, prod_col, year_col, per_col, dem_col)\n",
    "    \"\"\"\n",
    "    d = df.copy()\n",
    "    scen_col = _pick_col(d, [\"scenario\"])\n",
    "    prod_col = _pick_col(d, [\"product_name\",\"product\"])\n",
    "    year_col = _pick_col(d, [\"year\",\"Year\"], required=False)\n",
    "    per_col  = _pick_col(d, [\"period\",\"month\",\"Month\"])\n",
    "    dem_col  = _pick_col(d, [\"demand_units\",\"demand\",\"qty\",\"quantity\"])\n",
    "\n",
    "    d[per_col] = pd.to_numeric(d[per_col], errors=\"coerce\").astype(int)\n",
    "\n",
    "    if year_col is not None:\n",
    "        d = d.sort_values([scen_col, year_col, per_col])\n",
    "        d[\"ds\"] = pd.to_datetime(dict(\n",
    "            year=pd.to_numeric(d[year_col], errors=\"coerce\").astype(int),\n",
    "            month=d[per_col], day=1\n",
    "        ))\n",
    "    else:\n",
    "        d = d.sort_values([scen_col, per_col])\n",
    "        d[\"_yr_seq\"] = d.groupby([scen_col, per_col]).cumcount()\n",
    "        d[\"ds\"] = pd.to_datetime(dict(year=2000 + d[\"_yr_seq\"], month=d[per_col], day=1))\n",
    "\n",
    "    d[\"t_month\"] = d.groupby(scen_col)[\"ds\"].rank(method=\"dense\").astype(int) - 1\n",
    "    return d, scen_col, prod_col, year_col, per_col, dem_col\n",
    "\n",
    "#--------------------------------------------------------------------------------------------\n",
    "def normalize_time_strict(d, sc=\"scenario\", yc=\"Year\", mc=\"period\"):\n",
    "    \"\"\"Create a continuous month index per scenario.\"\"\"\n",
    "    d = d.copy()\n",
    "    d[yc] = d[yc].astype(int)\n",
    "    d[mc] = d[mc].astype(int)\n",
    "    months = (d[[sc, yc, mc]].drop_duplicates()\n",
    "              .sort_values([sc, yc, mc]).reset_index(drop=True))\n",
    "    months[\"t_month\"] = months.groupby(sc).cumcount()\n",
    "    return d.merge(months, on=[sc, yc, mc], how=\"left\")\n",
    "\n",
    "def smape_vec(y_true, y_pred):  # percent\n",
    "    y_true = np.asarray(y_true, float).ravel()\n",
    "    y_pred = np.asarray(y_pred, float).ravel()\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred)) / 2.0\n",
    "    return float(np.mean(np.abs(y_true - y_pred) / np.maximum(denom, 1e-8)) * 100.0)\n",
    "\n",
    "def mase_from_insample(y_insample, y_true, y_pred, m=12):\n",
    "    \"\"\"MASE per Hyndman & Koehler: needs INSAMPLE series for the seasonal naive denominator.\"\"\"\n",
    "    y_insample = np.asarray(y_insample, float).ravel()\n",
    "    y_true     = np.asarray(y_true,     float).ravel()\n",
    "    y_pred     = np.asarray(y_pred,     float).ravel()\n",
    "    if len(y_insample) <= m: \n",
    "        return np.nan\n",
    "    denom = np.mean(np.abs(y_insample[m:] - y_insample[:-m]))\n",
    "    num   = np.mean(np.abs(y_true - y_pred))\n",
    "    return float(num / np.maximum(denom, 1e-12))\n",
    "\n",
    "    \n",
    "def wape_percent(y_true, y_pred):  # percent\n",
    "    y_true = np.asarray(y_true, float).ravel()\n",
    "    y_pred = np.asarray(y_pred, float).ravel()\n",
    "    return float(100.0 * np.sum(np.abs(y_true - y_pred)) /\n",
    "                 np.maximum(np.sum(np.abs(y_true)), 1e-8))\n",
    "\n",
    "def mase(y_true, y_pred, seasonal_period=12):\n",
    "    \"\"\"\n",
    "    MASE = MAE(model) / MAE(naive seasonal, lag = seasonal_period).\n",
    "    Returns np.nan if there aren't enough points for the seasonal naive.\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=float).ravel()\n",
    "    y_pred = np.asarray(y_pred, dtype=float).ravel()\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    if len(y_true) <= seasonal_period:\n",
    "        return np.nan  # not enough data to compute seasonal naive\n",
    "    naive_err = np.abs(y_true[seasonal_period:] - y_true[:-seasonal_period])\n",
    "    denom = np.mean(naive_err)\n",
    "    if denom < 1e-8:\n",
    "        return np.inf  # or return 0.0 depending on your preference\n",
    "    return float(mae / denom)\n",
    "\n",
    "def wape(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=\"float64\")\n",
    "    y_pred = np.asarray(y_pred, dtype=\"float64\")\n",
    "    return float(np.sum(np.abs(y_true - y_pred)) / (np.sum(np.abs(y_true)) + 1e-8) * 100.0)\n",
    "\n",
    "def rmse_vec(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=\"float64\")\n",
    "    y_pred = np.asarray(y_pred, dtype=\"float64\")\n",
    "    return float(np.sqrt(np.mean((y_true - y_pred)**2)))\n",
    "\n",
    "\n",
    "\n",
    "##---------------------- here would be panel_for_scenario (region aggregation)  and panel_for_scenario_matrix_safe (your “safe” pivot)\n",
    "def panel_for_scenario(df, scen, products=PRODUCTS):\n",
    "    g = df.loc[df[\"scenario\"] == scen].copy()\n",
    "    if \"region\" in g.columns:\n",
    "        g = g.groupby([\"period\", \"product_name\"], as_index=False)[\"demand_units\"].sum()\n",
    "\n",
    "    pv = (g.assign(product_name=lambda x: x[\"product_name\"].astype(str).str.strip())\n",
    "            .pivot(index=\"period\", columns=\"product_name\", values=\"demand_units\")\n",
    "            .reindex(columns=products)\n",
    "            .sort_index())\n",
    "\n",
    "    if pv.isna().any().any():\n",
    "        raise ValueError(f\"NaNs after pivot for scenario '{scen}'.\")\n",
    "    return pv\n",
    "\n",
    "\n",
    "def panel_for_scenario_matrix(df, scen, prod_col, dem_col, impute=\"drop\"):\n",
    "    g = df[df[\"scenario\"] == scen].copy()\n",
    "    pt = (g.pivot_table(index=\"t_month\", columns=prod_col, values=dem_col, aggfunc=\"first\")\n",
    "            .reindex(columns=PRODUCTS))\n",
    "    if impute == \"drop\":\n",
    "        pt = pt.dropna(axis=0, how=\"any\")\n",
    "    elif impute == \"zero\":\n",
    "        pt = pt.fillna(0.0)\n",
    "    elif impute == \"ffill\":\n",
    "        pt = pt.sort_index().ffill().dropna(axis=0, how=\"any\")\n",
    "    else:\n",
    "        raise ValueError(\"impute must be 'drop', 'zero', or 'ffill'.\")\n",
    "    if pt.empty:\n",
    "        raise ValueError(f\"No complete months for scenario {scen} after imputation='{impute}'.\")\n",
    "    return pt.to_numpy(dtype=\"float64\")\n",
    "\n",
    "# ===============================\n",
    "# 3) Safe panel builder (T × m)\n",
    "# ===============================\n",
    "\n",
    "\n",
    "def panel_for_scenario_matrix_safe(df, scen, prod_col, dem_col,\n",
    "                                   time_col=\"t_month\", scen_col=\"scenario\",\n",
    "                                   products=None, impute=\"drop\"):\n",
    "    # 1) checks\n",
    "    if not hasattr(df, \"columns\"):\n",
    "        raise TypeError(f\"'df' must be a pandas DataFrame, got: {type(df)}\")\n",
    "    for col in (scen_col, prod_col, dem_col, time_col):\n",
    "        if col not in df.columns:\n",
    "            raise KeyError(f\"Column '{col}' not in df.columns.\")\n",
    "    if products is None:\n",
    "        products = list(df[prod_col].drop_duplicates())\n",
    "\n",
    "    # 2) filter + pivot\n",
    "    g = df.loc[df[scen_col] == scen, [time_col, prod_col, dem_col]].copy()\n",
    "    pt = (g.pivot_table(index=time_col, columns=prod_col,\n",
    "                        values=dem_col, aggfunc=\"first\")\n",
    "            .reindex(columns=products))\n",
    "\n",
    "    # 3) impute\n",
    "    if impute == \"drop\":\n",
    "        pt = pt.dropna()\n",
    "    elif impute == \"ffill\":\n",
    "        pt = pt.sort_index().ffill()\n",
    "    elif impute == \"zero\":\n",
    "        pt = pt.fillna(0.0)\n",
    "    else:\n",
    "        raise ValueError(\"impute must be 'drop' | 'ffill' | 'zero'\")\n",
    "\n",
    "    return pt\n",
    "\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 1) Canonical dataset builder\n",
    "# ===============================\n",
    "def make_df_use(df):\n",
    "    \"\"\"Aggregate (if region exists), ensure a scenario column,\n",
    "    and standardize column names expected by the pipeline.\"\"\"\n",
    "    Y  = \"Year\" if \"Year\" in df.columns else \"year\"\n",
    "    P  = \"period\" if \"period\" in df.columns else (\"month\" if \"month\" in df.columns else \"Month\")\n",
    "    PN = \"product_name\" if \"product_name\" in df.columns else (\"product\" if \"product\" in df.columns else \"product_id\")\n",
    "    V  = (\"demand_units\" if \"demand_units\" in df.columns else\n",
    "          (\"demand\" if \"demand\" in df.columns else\n",
    "           (\"qty\" if \"qty\" in df.columns else \"quantity\")))\n",
    "    d = df.copy()\n",
    "\n",
    "    if \"region\" in d.columns:\n",
    "        d = d.groupby([Y, P, PN], as_index=False)[V].sum()\n",
    "\n",
    "    if \"scenario\" not in d.columns:\n",
    "        d[\"scenario\"] = \"Baseline_Seasonal\"\n",
    "\n",
    "    d = d.rename(columns={Y:\"Year\", P:\"period\", PN:\"product_name\", V:\"demand_units\"})\n",
    "    d[\"Year\"]   = d[\"Year\"].astype(int)\n",
    "    d[\"period\"] = d[\"period\"].astype(int)\n",
    "    return d\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def add_calendar_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    df = ensure_time_columns(df).copy()  # guarantees Year/period/ds\n",
    "    df[\"date\"]  = df[\"ds\"]\n",
    "    df[\"month\"] = df[\"period\"].astype(int)\n",
    "    df[\"year\"]  = df[\"Year\"].astype(int)\n",
    "\n",
    "    # Seasonal encodings\n",
    "    df[\"m_sin\"] = np.sin(2*np.pi*df[\"month\"]/12.0)\n",
    "    df[\"m_cos\"] = np.cos(2*np.pi*df[\"month\"]/12.0)\n",
    "    return df\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fa25877e-1cb5-4f81-b236-4d134466879f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Panel shape: (60, 3) Any NaN? False\n",
      "Missing per product (pre-impute): {'iPhone': 0, 'iPad': 0, 'MacBook': 0}\n"
     ]
    }
   ],
   "source": [
    "###### CELL 3 — Canonical data + the 6 consistency steps (must run before any model)\n",
    "\n",
    "df_use = make_df_use(df)                 # your function\n",
    "df_use = ensure_time_columns_df(df_use)  # guarantees Year, period, ds\n",
    "\n",
    "df2 = normalize_time_strict(df_use)      # adds t_month\n",
    "SCEN = \"Baseline_Seasonal\"\n",
    "panel = panel_for_scenario_matrix_safe(\n",
    "    df2, SCEN,\n",
    "    prod_col=\"product_name\", dem_col=\"demand_units\",\n",
    "    time_col=\"t_month\", scen_col=\"scenario\",\n",
    "    products=PRODUCTS, impute=\"drop\"\n",
    ")\n",
    "print(\"Panel shape:\", panel.shape, \"Any NaN?\", panel.isna().any().any())\n",
    "\n",
    "pivot_raw = (df2[df2[\"scenario\"]==SCEN]\n",
    "             .pivot_table(index=\"t_month\", columns=\"product_name\", values=\"demand_units\", aggfunc=\"first\")\n",
    "             .reindex(columns=PRODUCTS))\n",
    "print(\"Missing per product (pre-impute):\", pivot_raw.isna().sum().to_dict())\n",
    "\n",
    "\n",
    "\n",
    "mat_full = panel.to_numpy(\"float64\")\n",
    "T = mat_full.shape[0]\n",
    "max_start = T - HORIZON\n",
    "anchors = list(range(LOOKBACK, max_start, max(1, (max_start-LOOKBACK)//4)))[-4:]\n",
    "t_end = anchors[-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "cb69713a-ee2d-406e-8fd2-74386faa0e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "##CELL4\n",
    "# =========================\n",
    "# XGBOOST: tabular dataset + trainer + predictor\n",
    "# =========================\n",
    "from collections import defaultdict\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "except ImportError as e:\n",
    "    raise ImportError(\"Please install xgboost: pip install xgboost\") from e\n",
    "\n",
    "def _month_one_hot(m):  # m in 1..12\n",
    "    oh = np.zeros(12, dtype=np.float32)\n",
    "    oh[int(m)-1] = 1.0\n",
    "    return oh\n",
    "\n",
    "def _flags_for_month(m):\n",
    "    # simple example flags to mirror your DL exogenous inputs\n",
    "    flag_launch  = 1.0 if int(m) == 9 else 0.0\n",
    "    flag_holiday = 1.0 if int(m) in (10,11,12) else 0.0\n",
    "    return np.array([flag_launch, flag_holiday], dtype=np.float32)\n",
    "\n",
    "def _panel_products(df, scen, products):\n",
    "    \"\"\"Return (period_seq, matrix T×m) for a scenario.\"\"\"\n",
    "    g = (df[df[\"scenario\"] == scen]\n",
    "           .groupby([\"Year\",\"period\",\"product_name\"], as_index=False)[\"demand_units\"]\n",
    "           .sum()\n",
    "           .sort_values([\"Year\",\"period\"]))\n",
    "    # we need the chronological month sequence (1..12 repeating)\n",
    "    period_seq = g[[\"Year\",\"period\"]].drop_duplicates().sort_values([\"Year\",\"period\"])[\"period\"].to_numpy()\n",
    "    pv = (g.pivot(index=[\"Year\",\"period\"], columns=\"product_name\", values=\"demand_units\")\n",
    "            .sort_index()\n",
    "            .reset_index(drop=True)\n",
    "            .reindex(columns=products))\n",
    "    M = pv.to_numpy(dtype=np.float32)\n",
    "    # fill any gaps with zeros (or ffill if you prefer)\n",
    "    M = np.nan_to_num(M, nan=0.0)\n",
    "    return period_seq.astype(int), M\n",
    "\n",
    "def build_tabular_dataset(df, lookback, horizon, products=PRODUCTS):\n",
    "    \"\"\"\n",
    "    Build rolling windows across *all scenarios*.\n",
    "    X features = flattened last L×m product block  +  one-hot(month_target) + flags(target_month)\n",
    "    y targets  = next h step for each product (we train one regressor per (product, h))\n",
    "    Returns: dict with\n",
    "        feat_windows: (N, L*m)    # common for all h\n",
    "        target_months: (N, horizon) ints in 1..12 for each sample & horizon\n",
    "        ys: dict[(prod_idx, h)] -> (N,)\n",
    "        meta: {'L':..., 'm':..., 'horizon':..., 'N':...}\n",
    "    \"\"\"\n",
    "    dfu = make_df_use(df)\n",
    "    scenarios = dfu[\"scenario\"].unique().tolist()\n",
    "\n",
    "    feat_list = []                         # flattened L×m window\n",
    "    target_months = []                     # (N, horizon)\n",
    "    ys = defaultdict(list)                 # per (j,h) target\n",
    "\n",
    "    for scen in scenarios:\n",
    "        period_seq, M = _panel_products(dfu, scen, products)  # T×m\n",
    "        T, m = M.shape\n",
    "        if T < lookback + horizon:  # not enough for this scenario\n",
    "            continue\n",
    "\n",
    "        for start in range(0, T - lookback - horizon + 1):\n",
    "            end = start + lookback\n",
    "            block = M[start:end, :]                            # (L, m)\n",
    "            feat_list.append(block.reshape(-1))                # (L*m,)\n",
    "\n",
    "            # compute target months for each horizon step\n",
    "            months_h = []\n",
    "            for h in range(1, horizon+1):\n",
    "                tgt_idx = end + (h - 1)\n",
    "                months_h.append(int(period_seq[tgt_idx]))\n",
    "                # push targets per product\n",
    "                for j in range(m):\n",
    "                    ys[(j, h)].append(float(M[tgt_idx, j]))\n",
    "            target_months.append(months_h)\n",
    "\n",
    "    if len(feat_list) == 0:\n",
    "        raise ValueError(\"No windows built. Check LOOKBACK/HORIZON versus data length.\")\n",
    "\n",
    "    X_base = np.asarray(feat_list, dtype=np.float32)          # (N, L*m)\n",
    "    target_months = np.asarray(target_months, dtype=int)      # (N, H)\n",
    "    N = X_base.shape[0]\n",
    "    L = lookback\n",
    "    m = len(products)\n",
    "\n",
    "    meta = {\"L\": L, \"m\": m, \"horizon\": horizon, \"N\": N}\n",
    "\n",
    "    # Convert dict of lists -> arrays\n",
    "    ys_arr = {k: np.asarray(v, dtype=np.float32) for k, v in ys.items()}\n",
    "\n",
    "    return X_base, target_months, ys_arr, meta\n",
    "\n",
    "def _build_features_for_h(X_base, target_months, h):\n",
    "    \"\"\"\n",
    "    Concatenate base window with exogenous for horizon step h:\n",
    "    X_h = [X_base, one_hot(month_h), flags(month_h)]\n",
    "    \"\"\"\n",
    "    N = X_base.shape[0]\n",
    "    months_h = target_months[:, h-1]                   # (N,)\n",
    "    oh = np.vstack([_month_one_hot(m) for m in months_h])      # (N,12)\n",
    "    fl = np.vstack([_flags_for_month(m) for m in months_h])    # (N,2)\n",
    "    return np.concatenate([X_base, oh, fl], axis=1)            # (N, L*m + 14)\n",
    "\n",
    "def train_xgb_forecasters(df, lookback, horizon, products=PRODUCTS):\n",
    "    \"\"\"\n",
    "    Trains one XGBRegressor per (product, horizon_step).\n",
    "    Uses a chronological 80/20 split over the constructed windows for quick validation.\n",
    "    Returns:\n",
    "      models: dict[(product_name, h)] -> XGBRegressor\n",
    "      quick_smape: dict[product_name] -> mean sMAPE over h on the validation slice\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from xgboost import XGBRegressor\n",
    "    except ImportError:\n",
    "        raise ImportError(\"Install xgboost: pip install xgboost\")\n",
    "\n",
    "    # build dataset\n",
    "    X_base, target_months, ys_arr, meta = build_tabular_dataset(df, lookback, horizon, products)\n",
    "    N = meta[\"N\"]\n",
    "    n_val = max(1, int(round(N * 0.2)))\n",
    "    n_tr  = N - n_val\n",
    "\n",
    "    models = {}\n",
    "    smape_per_prod = defaultdict(list)\n",
    "\n",
    "    # Use normalized products (and what’s really present in ys_arr)\n",
    "    prods_train = sorted({_norm_prod(p) for p in products})\n",
    "    # In some pipelines ys_arr is keyed by (prod,h) with prod’s original spelling; normalize a view:\n",
    "    # (If your ys_arr is already accessed by index only, this is benign)\n",
    "    # We will just rely on the 'products' list we passed to build_tabular_dataset.\n",
    "\n",
    "    for j, prod in enumerate(products):\n",
    "        prod_n = _norm_prod(prod)\n",
    "\n",
    "        for h in range(1, horizon+1):\n",
    "            X_h = _build_features_for_h(X_base, target_months, h)   # (N, L*m + 14)\n",
    "            y   = ys_arr[(j, h)]                                    # (N,)\n",
    "\n",
    "            Xtr, ytr = X_h[:n_tr], y[:n_tr]\n",
    "            Xvl, yvl = X_h[n_tr:],  y[n_tr:]\n",
    "\n",
    "            model = XGBRegressor(\n",
    "                n_estimators=400,\n",
    "                max_depth=4,\n",
    "                learning_rate=0.05,\n",
    "                subsample=0.9,\n",
    "                colsample_bytree=0.9,\n",
    "                reg_lambda=1.0,\n",
    "                objective=\"reg:squarederror\",\n",
    "                random_state=SEED,\n",
    "            )\n",
    "            model.fit(Xtr, ytr, eval_set=[(Xvl, yvl)], verbose=False)\n",
    "            models[(prod_n, h)] = model  # <-- store under NORMALIZED product key\n",
    "\n",
    "            if len(yvl) > 0:\n",
    "                yhat_vl = model.predict(Xvl)\n",
    "                sm = smape_vec(yvl, yhat_vl)\n",
    "                smape_per_prod[prod_n].append(sm)\n",
    "\n",
    "    quick_smape = {prod_n: float(np.mean(v)) if len(v) else np.nan\n",
    "                   for prod_n, v in smape_per_prod.items()}\n",
    "\n",
    "    print(\"XGB models trained for products (normalized):\", sorted({k[0] for k in models.keys()}))\n",
    "    return models, quick_smape\n",
    "\n",
    "\n",
    "def forecast_xgb(models, df, scen, lookback, horizon, products=PRODUCTS):\n",
    "    \"\"\"\n",
    "    Make an H×m forecast for a scenario using trained per-(product,h) models.\n",
    "    Uses the *last* lookback window from the scenario and rolls months forward.\n",
    "    \"\"\"\n",
    "    dfu = make_df_use(df).copy()\n",
    "    # normalize product names in the DF so pivot/selection aligns with model keys\n",
    "    dfu[\"product_name\"] = dfu[\"product_name\"].astype(str).str.strip()\n",
    "\n",
    "    # normalize the requested products order\n",
    "    products_n = [_norm_prod(p) for p in products]\n",
    "\n",
    "    # get last window block (L×m) over the chosen scenario\n",
    "    period_seq, M = _panel_products(dfu, scen, products_n)  # T×m (columns in products_n order)\n",
    "    T, m = M.shape\n",
    "    if T < lookback:\n",
    "        raise ValueError(f\"Not enough history for scenario={scen}: T={T} < L={lookback}\")\n",
    "\n",
    "    # base features (flattened last L×m)\n",
    "    block = M[-lookback:, :]                      # (L, m)\n",
    "    base = block.reshape(-1).astype(np.float32)   # (L*m,)\n",
    "\n",
    "    last_month = int(period_seq[-1])              # 1..12\n",
    "    rows = []\n",
    "    for h in range(1, horizon+1):\n",
    "        # roll calendar forward\n",
    "        target_month = ((last_month - 1 + h) % 12) + 1\n",
    "        feat = np.concatenate(\n",
    "            [base, _month_one_hot(target_month), _flags_for_month(target_month)],\n",
    "            axis=0\n",
    "        )[None, :]\n",
    "\n",
    "        preds = []\n",
    "        for j, prod in enumerate(products_n):\n",
    "            # try normalized key first\n",
    "            mdl = models.get((prod, h))\n",
    "            if mdl is None:\n",
    "                # (optional) backward-compat: if someone trained with original key\n",
    "                mdl = models.get((_norm_prod(prod), h))\n",
    "            if mdl is None:\n",
    "                raise KeyError(\n",
    "                    f\"XGB model not found for product '{prod}' at horizon h={h}. \"\n",
    "                    f\"Available keys sample: {list(models.keys())[:6]}...\"\n",
    "                )\n",
    "            preds.append(float(mdl.predict(feat)[0]))\n",
    "        rows.append(preds)\n",
    "\n",
    "    out = pd.DataFrame(rows, columns=products_n)\n",
    "    out.insert(0, \"horizon\", np.arange(1, horizon+1))\n",
    "    out.insert(0, \"scenario\", scen)\n",
    "    return out\n",
    "\n",
    "# Convenience wrappers used in your code\n",
    "def train_xgb(df_train):\n",
    "    return train_xgb_forecasters(df_train, LOOKBACK, HORIZON)[0]\n",
    "\n",
    "def predict_xgb_df(models_dict, df_train, scen, lookback, horizon):\n",
    "    return forecast_xgb(models_dict, df_train, scen, lookback, horizon)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "37ccd956-21df-46af-87ed-44cdcd64060d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #CELL 5 — SARIMAX (with exog, fixed columns) \n",
    "# # ---- SARIMAX (fixed exog dtype + stable columns) ----\n",
    "\n",
    "def _unpack_sarimax_record(rec):\n",
    "    \"\"\"\n",
    "    Accepts a SARIMAX model record in either format:\n",
    "      • dict: {\"res\": <results>, \"log1p\": bool}\n",
    "      • tuple/list: (<results>, <optional bool log1p>)\n",
    "    Returns (res, log1p_bool).\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    # dict case\n",
    "    if isinstance(rec, dict):\n",
    "        res = rec.get(\"res\")\n",
    "        if res is None:\n",
    "            raise KeyError(\"SARIMAX record dict missing 'res'.\")\n",
    "        return res, bool(rec.get(\"log1p\", False))\n",
    "\n",
    "    # tuple/list case\n",
    "    if isinstance(rec, (tuple, list)):\n",
    "        if len(rec) == 2:\n",
    "            return rec[0], bool(rec[1])\n",
    "        elif len(rec) >= 1:\n",
    "            # if only the results are stored, assume no log1p\n",
    "            return rec[0], False\n",
    "\n",
    "    raise TypeError(f\"Unrecognized SARIMAX record type: {type(rec)} with value: {rec}\")\n",
    "\n",
    "\n",
    "def sarimax_forecast_df(models, df, scen, horizon):\n",
    "    \"\"\"\n",
    "    Build an H×m forecast DataFrame from stored SARIMAX fits.\n",
    "    Works whether models[(scen, prod)] is a dict or a tuple.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    rows = []\n",
    "    used_key_shape = None\n",
    "\n",
    "    for prod in PRODUCTS:\n",
    "        # Prefer (scenario, product) key; fall back to product-only if needed\n",
    "        if (scen, prod) in models:\n",
    "            rec = models[(scen, prod)]\n",
    "            used_key_shape = \"(scenario, product)\"\n",
    "        elif prod in models:\n",
    "            rec = models[prod]\n",
    "            used_key_shape = \"(product)\"\n",
    "        else:\n",
    "            raise KeyError(f\"SARIMAX model not found for product '{prod}' (scenario='{scen}').\")\n",
    "\n",
    "        res, log1p = _unpack_sarimax_record(rec)\n",
    "\n",
    "        # Forecast next horizon steps\n",
    "        fc = res.get_forecast(steps=horizon).predicted_mean  # pandas Series or array\n",
    "        yhat = np.asarray(fc, dtype=\"float64\").reshape(-1)\n",
    "\n",
    "        # invert log1p if necessary, clip negatives\n",
    "        if log1p:\n",
    "            yhat = np.expm1(yhat)\n",
    "        yhat = np.clip(yhat, 0.0, None)\n",
    "\n",
    "        rows.append(yhat)\n",
    "\n",
    "    # stack to H×m and return DataFrame\n",
    "    arr = np.vstack(rows).T  # shape (H, m) after stacking per-product rows\n",
    "    out = pd.DataFrame(arr, columns=PRODUCTS)\n",
    "    out.insert(0, \"horizon\", np.arange(1, horizon + 1))\n",
    "    out.insert(0, \"scenario\", scen)\n",
    "    return out\n",
    "\n",
    "\n",
    "def predict_sarimax_df(model_obj, df_train, scen, lookback, horizon):\n",
    "    # Pass-through wrapper to keep your call sites unchanged\n",
    "    return sarimax_forecast_df(model_obj, df_train, scen=scen, horizon=horizon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "812febb3-04db-45c1-808a-201ad7315f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ensure_canonical(df):\n",
    "    # Your normalizer from earlier\n",
    "    return ensure_time_columns_df(df)\n",
    "\n",
    "def _panel_for_plot(df, scen, products):\n",
    "    d = df.copy()\n",
    "    if \"region\" in d.columns:\n",
    "        d = (d.groupby([\"scenario\", \"Year\", \"period\", \"product_name\"], as_index=False)\n",
    "               [\"demand_units\"].sum())\n",
    "    d = d[d[\"scenario\"] == scen].copy()\n",
    "    d = d.sort_values([\"Year\", \"period\", \"product_name\"])\n",
    "    d[\"t_month\"] = (d[\"Year\"] - d[\"Year\"].min())*12 + (d[\"period\"] - 1)\n",
    "    pt = (d.pivot_table(index=\"t_month\", columns=\"product_name\",\n",
    "                        values=\"demand_units\", aggfunc=\"first\")\n",
    "            .reindex(columns=products))\n",
    "    return pt, d\n",
    "\n",
    "def plot_sarimax_forecast(models, df, scen, products, horizon=12, title=None):\n",
    "    df = _ensure_canonical(df)\n",
    "    pt, d = _panel_for_plot(df, scen, products)\n",
    "\n",
    "    # Build future month dummies (m_2..m_12) for the next H steps\n",
    "    last_period = int(d.loc[d[\"scenario\"]==scen, \"period\"].max())\n",
    "    fut_months = ((np.arange(1, horizon+1) + last_period - 1) % 12) + 1\n",
    "    exog_cols = [f\"m_{m}\" for m in range(2,13)]\n",
    "    fut_cat = pd.Categorical(fut_months, categories=np.arange(1,13), ordered=True)\n",
    "    ex_fut_df = pd.get_dummies(fut_cat, drop_first=True).astype(\"float64\")\n",
    "    ex_fut_df.columns = exog_cols\n",
    "    ex_fut_df = ex_fut_df.reindex(columns=exog_cols, fill_value=0.0)\n",
    "    ex_fut = ex_fut_df.values  # (H, 11)\n",
    "\n",
    "    nrows = len(products)\n",
    "    fig, axes = plt.subplots(nrows, 1, figsize=(11, 2.6*nrows), sharex=True)\n",
    "    if nrows == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    x_hist = pt.index.values\n",
    "    for ax, prod in zip(axes, products):\n",
    "        series = pt[prod].astype(\"float64\").values\n",
    "        ax.plot(x_hist, series, lw=2, label=\"Actual\")\n",
    "\n",
    "        info = models[(scen, prod)]\n",
    "        res  = info[\"res\"]\n",
    "        # Forecast with aligned exogenous\n",
    "        fc    = res.get_forecast(steps=horizon, exog=ex_fut)\n",
    "        mean  = fc.predicted_mean\n",
    "        ci    = fc.conf_int(alpha=0.05)  # DataFrame with 2 cols\n",
    "\n",
    "        # If log1p was used, invert\n",
    "        if info.get(\"log1p\", True):\n",
    "            mean = np.expm1(mean)\n",
    "            ci   = np.expm1(ci)\n",
    "\n",
    "        x_fc = np.arange(x_hist[-1] + 1, x_hist[-1] + 1 + horizon)\n",
    "        ax.plot(x_fc, mean.values, lw=2, linestyle=\"--\", label=\"Forecast\")\n",
    "        ax.fill_between(x_fc, ci.iloc[:,0].values, ci.iloc[:,1].values,\n",
    "                        alpha=0.2, label=\"95% CI\")\n",
    "\n",
    "        ax.set_title(f\"{prod} – {scen}\")\n",
    "        ax.set_ylabel(\"Demand (units)\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    axes[-1].set_xlabel(\"t_month (monotone index)\")\n",
    "    if title:\n",
    "        fig.suptitle(title, y=0.98)\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc=\"upper right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7a0626eb-d5f0-49bf-8f40-b7b94d3f5cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Prophet trainer + SAFE forecaster (per-scenario, per-product) =====\n",
    "from prophet import Prophet\n",
    "\n",
    "def _to_month_end(year: int, month: int):\n",
    "    return pd.Period(f\"{int(year)}-{int(month):02d}\", freq=\"M\").to_timestamp()\n",
    "\n",
    "def _prophet_frame(g: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Expect columns: Year, period, demand_units\n",
    "    Return: ds, y, and regressor columns used both in fit and predict.\n",
    "    \"\"\"\n",
    "    dfp = g.sort_values([\"Year\",\"period\"]).copy()\n",
    "    dfp[\"ds\"] = [_to_month_end(y, m) for y, m in zip(dfp[\"Year\"], dfp[\"period\"])]\n",
    "    dfp[\"y\"]  = dfp[\"demand_units\"].astype(float)\n",
    "    # IMPORTANT: keep regressor names consistent with predict()\n",
    "    dfp[\"flag_launch\"]  = (dfp[\"period\"] == 9).astype(int)\n",
    "    dfp[\"flag_holiday\"] = dfp[\"period\"].isin([10,11,12]).astype(int)\n",
    "    return dfp[[\"ds\",\"y\",\"flag_launch\",\"flag_holiday\"]]\n",
    "\n",
    "def _prophet_future(last_ds: pd.Timestamp, horizon: int, add_regressors=None) -> pd.DataFrame:\n",
    "    regs = list(add_regressors) if add_regressors is not None else [\"flag_launch\",\"flag_holiday\"]\n",
    "    # 'ME' = MonthEnd (new pandas alias replacing 'M')\n",
    "    future_ds = pd.date_range(last_ds + pd.offsets.MonthEnd(1), periods=horizon, freq=\"ME\")\n",
    "    fut = pd.DataFrame({\"ds\": future_ds})\n",
    "    fut[\"month\"] = fut[\"ds\"].dt.month.astype(int)\n",
    "    if \"flag_launch\" in regs:\n",
    "        fut[\"flag_launch\"]  = (fut[\"month\"] == 9).astype(int)\n",
    "    if \"flag_holiday\" in regs:\n",
    "        fut[\"flag_holiday\"] = fut[\"month\"].isin([10,11,12]).astype(int)\n",
    "    keep_cols = [\"ds\"] + [r for r in regs if r in fut.columns]\n",
    "    return fut[keep_cols]\n",
    "\n",
    "\n",
    "def fit_prophet_all(df: pd.DataFrame, regressors=(\"flag_launch\",\"flag_holiday\")) -> dict:\n",
    "    \"\"\"\n",
    "    Train one Prophet per (scenario, product).\n",
    "    Returns dict keyed by (scenario, product) -> fitted model.\n",
    "    \"\"\"\n",
    "    d = make_df_use(df)\n",
    "    models = {}\n",
    "    for scen in d[\"scenario\"].unique():\n",
    "        for prod in d[\"product_name\"].unique():\n",
    "            g = d[(d[\"scenario\"]==scen) & (d[\"product_name\"]==prod)]\n",
    "            if len(g) < 8:\n",
    "                continue\n",
    "            dfp = _prophet_frame(g)\n",
    "            m = Prophet(yearly_seasonality=True, weekly_seasonality=False,\n",
    "                        daily_seasonality=False, seasonality_mode=\"multiplicative\")\n",
    "            for r in regressors:  # add exactly the same names as in dfp\n",
    "                m.add_regressor(r)\n",
    "            m.fit(dfp)\n",
    "            models[(scen, prod)] = m\n",
    "    return models\n",
    "\n",
    "def forecast_prophet_safe(models: dict, df: pd.DataFrame, scen: str, horizon: int,\n",
    "                          products=PRODUCTS, regressors=(\"flag_launch\",\"flag_holiday\")) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Safe forecaster: if a (scen, prod) model is missing, it fits on-the-fly.\n",
    "    Ensures future has the SAME regressors used in training.\n",
    "    \"\"\"\n",
    "    d = make_df_use(df)\n",
    "    g_all = d[d[\"scenario\"]==scen].sort_values([\"Year\",\"period\"])\n",
    "    if g_all.empty:\n",
    "        raise ValueError(f\"No data for scenario '{scen}'.\")\n",
    "    last_ds = _to_month_end(int(g_all[\"Year\"].iloc[-1]), int(g_all[\"period\"].iloc[-1]))\n",
    "\n",
    "    rows = {}\n",
    "    for prod in products:\n",
    "        key = (scen, prod)\n",
    "        if key not in models:\n",
    "            # Fit on the fly if missing\n",
    "            g = d[(d[\"scenario\"]==scen) & (d[\"product_name\"]==prod)]\n",
    "            if g.empty:\n",
    "                rows[prod] = [np.nan]*horizon\n",
    "                continue\n",
    "            dfp = _prophet_frame(g)\n",
    "            m = Prophet(yearly_seasonality=True, weekly_seasonality=False,\n",
    "                        daily_seasonality=False, seasonality_mode=\"multiplicative\")\n",
    "            for r in regressors:\n",
    "                m.add_regressor(r)\n",
    "            m.fit(dfp)\n",
    "            models[key] = m\n",
    "        m = models[key]\n",
    "        fut = _prophet_future(last_ds, horizon, add_regressors=regressors)  # <- now supported\n",
    "        fc = m.predict(fut)[[\"ds\",\"yhat\"]].sort_values(\"ds\").reset_index(drop=True)\n",
    "        rows[prod] = fc[\"yhat\"].to_numpy(dtype=\"float64\")\n",
    "\n",
    "    out = pd.DataFrame({p: rows[p] for p in products})\n",
    "    out.insert(0, \"horizon\", np.arange(1, horizon+1))\n",
    "    out.insert(0, \"scenario\", scen)\n",
    "    return out\n",
    "\n",
    "# wrappers used elsewhere in your code\n",
    "def train_prophet(df_train): \n",
    "    return fit_prophet_all(df_train)\n",
    "\n",
    "def predict_prophet_df(model_obj: dict, df_train: pd.DataFrame, scen: str, lookback: int, horizon: int) -> pd.DataFrame:\n",
    "    return forecast_prophet_safe(model_obj, df_train, scen=scen, horizon=horizon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e331d4d7-c89f-403c-a86a-d661bc4a9e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5) Simple ensemble\n",
    "def blend_forecasts(forecasts, weights=None, on=(\"scenario\",\"horizon\")):\n",
    "    if not forecasts:\n",
    "        raise ValueError(\"No forecasts provided.\")\n",
    "    if weights is None:\n",
    "        weights = [1.0/len(forecasts)]*len(forecasts)\n",
    "    if len(weights) != len(forecasts):\n",
    "        raise ValueError(\"weights length must match forecasts length.\")\n",
    "    wsum = float(sum(weights))\n",
    "    if wsum <= 0:\n",
    "        raise ValueError(\"Sum of weights must be positive.\")\n",
    "\n",
    "    # Merge all on keys to guarantee row alignment\n",
    "    key_cols = list(on)\n",
    "    out = forecasts[0][key_cols].copy()\n",
    "    for p in PRODUCTS:\n",
    "        out[p] = 0.0\n",
    "\n",
    "    for f, w in zip(forecasts, weights):\n",
    "        # Ensure required columns exist\n",
    "        missing = set(key_cols + list(PRODUCTS)) - set(f.columns)\n",
    "        if missing:\n",
    "            raise KeyError(f\"Forecast is missing columns: {missing}\")\n",
    "        # Align rows by keys\n",
    "        f_aligned = out[key_cols].merge(\n",
    "            f[key_cols + list(PRODUCTS)],\n",
    "            on=key_cols, how=\"left\", validate=\"one_to_one\"\n",
    "        )\n",
    "        for p in PRODUCTS:\n",
    "            out[p] += (w * f_aligned[p].astype(\"float64\"))\n",
    "\n",
    "    for p in PRODUCTS:\n",
    "        out[p] /= wsum\n",
    "        # Optional: non-negative clamp for demands\n",
    "        # out[p] = np.clip(out[p], 0, None)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0d5d7a33-46a7-4a2d-adb9-74d500e741bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_ensemble(fcs, smapes, temp=1.0, floor=1e-2):\n",
    "    \"\"\"\n",
    "    fcs:    dict name -> ndarray (H, m) with identical shapes\n",
    "    smapes: dict name -> float (lower better)\n",
    "    temp:   temperature; <1 sharpens, >1 flattens weights\n",
    "    floor:  minimum SMAPE used to avoid runaway weights\n",
    "    \"\"\"\n",
    "    if not fcs:\n",
    "        raise ValueError(\"No forecasts provided.\")\n",
    "    names = list(fcs.keys())\n",
    "    shapes = {k: fcs[k].shape for k in names}\n",
    "    if len(set(shapes.values())) != 1:\n",
    "        raise ValueError(f\"All arrays must share the same shape, got {shapes}\")\n",
    "    # inverse-error with soft floor\n",
    "    inv = {k: 1.0 / max(float(smapes[k]), floor) for k in names}\n",
    "    # temperature scaling\n",
    "    inv_t = {k: v**(1.0/temp) for k, v in inv.items()}\n",
    "    Z = sum(inv_t.values())\n",
    "    w = {k: v/Z for k, v in inv_t.items()}\n",
    "    ens = sum(w[k] * fcs[k] for k in names)\n",
    "    return ens, w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "16f5c39f-71cc-44d8-ac8a-7173fbc912f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DROP-IN TRAINERS (minimal) ===\n",
    "# Fits one SARIMAX per (scenario, product) and returns:\n",
    "#   { (scenario, product): {\"res\": results_obj, \"log1p\": bool} }\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def fit_sarimax_all(df, order=(0,1,1), seasonal_order=(0,1,1,12), force_log1p=None):\n",
    "\n",
    "    from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "    d = make_df_use(df)\n",
    "    d = normalize_time_strict(d) if \"t_month\" not in d.columns else d.copy()\n",
    "    models = {}\n",
    "\n",
    "    for scen, g in d.groupby(\"scenario\"):\n",
    "        for prod, gp in g.groupby(\"product_name\"):\n",
    "            gp = gp.sort_values([\"Year\",\"period\"])\n",
    "            y  = gp[\"demand_units\"].to_numpy(dtype=\"float64\")\n",
    "\n",
    "            # log1p only if zeros/small values present (keeps your downstream code happy)\n",
    "            if force_log1p is None:\n",
    "                use_log = np.any(y <= 0) or np.nanmin(y) < 5\n",
    "            else:\n",
    "                use_log = bool(force_log1p)\n",
    "            z = np.log1p(y) if use_log else y\n",
    "        \n",
    "\n",
    "            try:\n",
    "                mod = SARIMAX(\n",
    "                    z, order=order, seasonal_order=seasonal_order,\n",
    "                    enforce_stationarity=False, enforce_invertibility=False\n",
    "                )\n",
    "                res = mod.fit(disp=False)\n",
    "            except Exception:\n",
    "                # last-resort simple model if SARIMAX fails\n",
    "                z = pd.Series(z).ffill().bfill().to_numpy()\n",
    "                mod = SARIMAX(\n",
    "                    z, order=(0,1,1), seasonal_order=(0,1,1,12),\n",
    "                    enforce_stationarity=False, enforce_invertibility=False\n",
    "                )\n",
    "                res = mod.fit(disp=False)\n",
    "\n",
    "            models[(scen, str(prod))] = {\"res\": res, \"log1p\": use_log}\n",
    "\n",
    "    return models\n",
    "\n",
    "\n",
    "# Fits one Prophet per (scenario, product) and returns:\n",
    "#   { (scenario, product): fitted_prophet_model }\n",
    "def fit_prophet_all(df):\n",
    "    try:\n",
    "        from prophet import Prophet  # pip install prophet\n",
    "    except Exception:\n",
    "        # Fall back to fbprophet import name if needed\n",
    "        from fbprophet import Prophet\n",
    "\n",
    "    import pandas as pd\n",
    "    d = make_df_use(df)\n",
    "\n",
    "    # build month-end timestamps for Prophet\n",
    "    def _to_ds(row):\n",
    "        # MonthEnd to get stable month-end dates\n",
    "        return pd.Timestamp(year=int(row[\"Year\"]), month=int(row[\"period\"]), day=1) + pd.offsets.MonthEnd(0)\n",
    "\n",
    "    d[\"ds\"] = d.apply(_to_ds, axis=1)\n",
    "    d = d.sort_values([\"scenario\",\"product_name\",\"ds\"])\n",
    "    models = {}\n",
    "\n",
    "    for (scen, prod), gp in d.groupby([\"scenario\",\"product_name\"]):\n",
    "        m = Prophet(\n",
    "            yearly_seasonality=True,\n",
    "            weekly_seasonality=False,\n",
    "            daily_seasonality=False,\n",
    "            seasonality_mode=\"additive\",\n",
    "        )\n",
    "        # add a simple monthly seasonality (helps with strong month effects)\n",
    "        m.add_seasonality(name=\"monthly\", period=12, fourier_order=6)\n",
    "\n",
    "        dfp = gp[[\"ds\",\"demand_units\"]].rename(columns={\"demand_units\":\"y\"}).copy()\n",
    "        # Prophet requires y to be finite\n",
    "        dfp[\"y\"] = pd.to_numeric(dfp[\"y\"], errors=\"coerce\").fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "\n",
    "        m.fit(dfp)\n",
    "        models[(scen, str(prod))] = m\n",
    "\n",
    "    return models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b9c9d5cd-c7e6-436c-a1aa-5025557266a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 7 — Trainers & Forecasters (thin wrappers)\n",
    "def train_xgb(df_train):     return train_xgb_forecasters(df_train, LOOKBACK, HORIZON)[0]\n",
    "def train_sarimax(df_train): return fit_sarimax_all(df_train, order=(0,1,1), seasonal_order=(0,1,1,12))\n",
    "def train_prophet(df_train): return fit_prophet_all(df_train)\n",
    "\n",
    "def predict_xgb_df(model_obj, df_train, scen, lookback, horizon):\n",
    "    return forecast_xgb(model_obj, df_train, scen=scen, lookback=lookback, horizon=horizon)\n",
    "\n",
    "def predict_sarimax_df(model_obj, df_train, scen, lookback, horizon):\n",
    "    return sarimax_forecast_df(model_obj, df_train, scen=scen, horizon=horizon)\n",
    "\n",
    "def predict_prophet_df(model_obj, df_train, scen, lookback, horizon):\n",
    "    return forecast_prophet_safe(model_obj, df_train, scen=scen, horizon=horizon)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "de7c295c-c34f-4d37-a147-12a9bd84878f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB models trained for products (normalized): ['MacBook', 'iPad', 'iPhone']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:21:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:21:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:21:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:21:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:21:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:21:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:21:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:21:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:21:40 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:21:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:21:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:21:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:21:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:21:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:21:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:21:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:21:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:21:56 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:21:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:21:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:21:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:22:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:22:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:22:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:22:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:22:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:22:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:22:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:22:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:22:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:22:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:22:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:22:14 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:22:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:22:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:22:19 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running forecasts for scenario 'Baseline_Seasonal' using t_end=56\n",
      "\n",
      "--- Sanity check: median/mean comparison ---\n",
      "true     median=   40772.683 mean=   69255.273 | ratio= 1.00\n",
      "Prophet  median=   40417.191 mean=   66453.488 | ratio= 0.99\n",
      "SARIMAX  median=   37732.883 mean=   60548.037 | ratio= 0.93\n",
      "XGB      median=   40763.348 mean=   69277.587 | ratio= 1.00\n",
      "true  median: 40772.68317  mean: 69255.27349444444\n",
      "Prophet  median: 40417.19050087416  mean: 66453.48770139979\n",
      "SARIMAX  median: 37732.88340564829  mean: 60548.037004197664\n",
      "XGB  median: 40763.34765625  mean: 69277.58680555556\n"
     ]
    }
   ],
   "source": [
    "# CELL 8 — Last-fold arrays (for scale sanity)\n",
    "\n",
    "df2    = normalize_time_strict(df_use) # adds t_month\n",
    "SCEN = \"Baseline_Seasonal\"\n",
    "panel = panel_for_scenario_matrix_safe(\n",
    "    df2, SCEN,\n",
    "    prod_col=\"product_name\", dem_col=\"demand_units\",\n",
    "    time_col=\"t_month\", scen_col=\"scenario\",\n",
    "    products=PRODUCTS, impute=\"drop\"\n",
    ")\n",
    "mat_full = panel.to_numpy(\"float64\")\n",
    "T = mat_full.shape[0]\n",
    "max_start = T - HORIZON\n",
    "anchors = list(range(LOOKBACK, max_start, max(1, (max_start-LOOKBACK)//4)))[-4:]\n",
    "t_end = anchors[-1]\n",
    "\n",
    "# ===============================================\n",
    "#  last_fold_arrays – consistent across all models\n",
    "# ===============================================\n",
    "\n",
    "\n",
    "def last_fold_arrays(models_dict, df_use, scen, lookback, horizon):\n",
    "    \"\"\"\n",
    "    For the last validation fold:\n",
    "      • builds the aligned panel (t_month index)\n",
    "      • slices the same df_train for all models\n",
    "      • forecasts horizon months ahead\n",
    "      • returns a dict of arrays {model_name: np.ndarray}\n",
    "        including 'true' ground trut\n",
    "    \"\"\"\n",
    "    # 1️⃣ Normalize & build panel\n",
    "    df2 = normalize_time_strict(df_use)\n",
    "    pt = panel_for_scenario_matrix_safe(\n",
    "        df2, scen,\n",
    "        prod_col=\"product_name\",\n",
    "        dem_col=\"demand_units\",\n",
    "        time_col=\"t_month\",\n",
    "        scen_col=\"scenario\",\n",
    "        products=PRODUCTS,\n",
    "        impute=\"drop\"\n",
    "    )\n",
    "    mat_full = pt.to_numpy(dtype=\"float64\")\n",
    "    T = mat_full.shape[0]\n",
    "\n",
    "    if T < lookback + horizon + 1:\n",
    "        raise ValueError(f\"Not enough history T={T} for lookback={lookback}, horizon={horizon}\")\n",
    "\n",
    "    # 2️⃣ Pick last anchor (same for all models)\n",
    "    max_start = T - horizon\n",
    "    anchors = list(range(lookback, max_start, max(1, (max_start - lookback)//4)))[-4:]\n",
    "    t_end = anchors[-1]\n",
    "\n",
    "    # 3️⃣ Training data (same slice for all)\n",
    "    df_train = df2[df2[\"t_month\"] < t_end].copy()\n",
    "    y_true = mat_full[t_end : t_end + horizon, :]\n",
    "\n",
    "    # 4️⃣ Forecasts\n",
    "    out = {\"true\": y_true}\n",
    "    print(f\"\\nRunning forecasts for scenario '{scen}' using t_end={t_end}\")\n",
    "\n",
    "    out[\"Prophet\"] = predict_prophet_df(\n",
    "        models_dict[\"Prophet\"], df_train, scen, lookback, horizon\n",
    "    )[PRODUCTS].to_numpy(dtype=\"float64\")\n",
    "\n",
    "    out[\"SARIMAX\"] = predict_sarimax_df(\n",
    "        models_dict[\"SARIMAX\"], df_train, scen, lookback, horizon\n",
    "    )[PRODUCTS].to_numpy(dtype=\"float64\")\n",
    "\n",
    "    out[\"XGB\"] = predict_xgb_df(\n",
    "        models_dict[\"XGB\"], df_train, scen, lookback, horizon\n",
    "    )[PRODUCTS].to_numpy(dtype=\"float64\")\n",
    "\n",
    "    # 5️⃣ Scale sanity checks\n",
    "    def sanity_report(name, y_true, y_pred):\n",
    "        true_med = np.median(y_true)\n",
    "        pred_med = np.median(y_pred)\n",
    "        ratio = (pred_med + 1e-9) / (true_med + 1e-9)\n",
    "        print(f\"{name:8s} median={pred_med:12.3f} mean={np.mean(y_pred):12.3f} | ratio={ratio:5.2f}\")\n",
    "        if ratio < 0.25 or ratio > 4.0:\n",
    "            print(f\"⚠️  Warning: {name} off-scale (median ratio={ratio:.2f})\")\n",
    "\n",
    "    print(\"\\n--- Sanity check: median/mean comparison ---\")\n",
    "    sanity_report(\"true\", y_true, y_true)\n",
    "    for name in [\"Prophet\", \"SARIMAX\", \"XGB\"]:\n",
    "        sanity_report(name, y_true, out[name])\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "models_now = {\n",
    "    \"XGB\":     train_xgb(df_use),\n",
    "    \"SARIMAX\": train_sarimax(df_use),\n",
    "    \"Prophet\": train_prophet(df_use),\n",
    "}\n",
    "\n",
    "arrs = last_fold_arrays(models_now, df_use, SCEN, LOOKBACK, HORIZON)\n",
    "for k, v in arrs.items():\n",
    "    print(k, \" median:\", float(np.median(v)), \" mean:\", float(np.mean(v)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "114be3c0-9b3a-4ed9-92b0-2adf6ecc1ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            scenario  horizon         iPhone          iPad       MacBook\n",
      "0  Baseline_Seasonal        1  106478.249829  37732.883406  35247.414261\n",
      "1  Baseline_Seasonal        2  105585.126329  35634.960756  33774.171691\n",
      "2  Baseline_Seasonal        3  115378.849129  38691.347536  36409.330101\n",
      "SARIMAX med/mean: 37732.88340564829 60548.037004197664\n"
     ]
    }
   ],
   "source": [
    "# 1) Train SARIMAX (if not already done in this session)\n",
    "sarimax_models = fit_sarimax_all(\n",
    "    df,\n",
    "    order=(0,1,1),\n",
    "    seasonal_order=(0,1,1,12)\n",
    ")\n",
    "\n",
    "# 2) Forecast next H steps\n",
    "sarimax_fc = sarimax_forecast_df(sarimax_models, df, scen=\"Baseline_Seasonal\", horizon=HORIZON)\n",
    "print(sarimax_fc.head())\n",
    "\n",
    "# 3) Quick sanity: values should now be on the same scale as 'true'\n",
    "print(\"SARIMAX med/mean:\",\n",
    "      np.median(sarimax_fc[PRODUCTS].to_numpy()),\n",
    "      np.mean(sarimax_fc[PRODUCTS].to_numpy()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1fdf65ab-f15d-4ac4-b2d5-2e8cf1b25434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fold_anchors_safe(T, lookback, horizon, n_folds):\n",
    "    \"\"\"\n",
    "    Choose fold end points (t_end) in [lookback+horizon, T-horizon]\n",
    "    so the training slice df_train (t_month < t_end) has >= lookback+horizon months.\n",
    "    \"\"\"\n",
    "    lo = lookback + horizon\n",
    "    hi = T - horizon\n",
    "    if hi < lo:\n",
    "        # not enough total data for even one safe fold\n",
    "        return []\n",
    "    # spread n_folds anchors between lo..hi (inclusive), unique & sorted\n",
    "    anchors = np.unique(np.linspace(lo, hi, num=n_folds, dtype=int))\n",
    "    return anchors.tolist()\n",
    "\n",
    "def _has_enough_windows(df_train, lookback, horizon, scen):\n",
    "    \"\"\"\n",
    "    Returns True if df_train has at least one rolling window for (L=lookback, H=horizon)\n",
    "    for the given scenario. Accepts either an already-normalized frame (with t_month)\n",
    "    or a raw frame (with Year/period).\n",
    "    \"\"\"\n",
    "    d2 = make_df_use(df_train)\n",
    "    if \"t_month\" not in d2.columns:\n",
    "        d2 = normalize_time_strict(d2)\n",
    "\n",
    "    g = d2[d2[\"scenario\"] == scen]\n",
    "    if g.empty:\n",
    "        return False\n",
    "\n",
    "    T_train = g[\"t_month\"].nunique()\n",
    "    return (T_train - lookback - horizon + 1) > 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b84993af-6833-4692-97b9-37c3dc72a17e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB models trained for products (normalized): ['MacBook', 'iPad', 'iPhone']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:22:25 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:22:26 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:22:26 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:22:27 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:22:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:22:29 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:22:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:22:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:22:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:22:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:22:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:22:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:22:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:22:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:22:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:22:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:22:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:22:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:22:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:22:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:22:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:23:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:23:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:23:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:23:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:23:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:23:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:23:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:23:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:23:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:23:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:23:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:23:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:23:18 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:23:18 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:23:33 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB models trained for products (normalized): ['MacBook', 'iPad', 'iPhone']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:23:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:23:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:23:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:23:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:23:44 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:23:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:23:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:23:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:23:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:23:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:23:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:23:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:23:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:23:56 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:23:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:24:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:24:00 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:24:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:24:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:24:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:24:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:24:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:24:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:24:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:24:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:24:13 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:24:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:24:15 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:24:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:24:18 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:24:18 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:24:20 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:24:20 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:24:22 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:24:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:24:24 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB models trained for products (normalized): ['MacBook', 'iPad', 'iPhone']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:24:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:24:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:24:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:24:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:24:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:24:40 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:24:40 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:24:43 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:24:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:24:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:24:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:24:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:24:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:24:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:24:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:24:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:24:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:24:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:24:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:24:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:24:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:25:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:25:00 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:25:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:25:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:25:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:25:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:25:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:25:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:25:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:25:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:25:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:25:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:25:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:25:14 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:25:15 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per-fold results:\n",
      "   fold_end_t    model       sMAPE  MASE        WAPE           RMSE\n",
      "0          15      XGB    8.056331   NaN    7.924637    6565.433669\n",
      "1          15  SARIMAX    0.558536   NaN    0.598329     578.461417\n",
      "2          15  Prophet  125.803382   NaN  507.068968  605189.987024\n",
      "3          36      XGB   11.979984   NaN    8.117446    5403.705684\n",
      "4          36  SARIMAX   15.004968   NaN   14.923451    9953.385069\n",
      "5          36  Prophet   20.926909   NaN   21.752561   17086.732967\n",
      "6          57      XGB    3.326239   NaN    2.270703    1751.774326\n",
      "7          57  SARIMAX    0.105402   NaN    0.069269      63.372073\n",
      "8          57  Prophet    6.058857   NaN    5.177377    5375.861443\n",
      "\n",
      "Model averages:\n",
      "             sMAPE  MASE        WAPE           RMSE\n",
      "model                                              \n",
      "SARIMAX   5.222969   NaN    5.197016    3531.739520\n",
      "XGB       7.787518   NaN    6.104262    4573.637893\n",
      "Prophet  50.929716   NaN  177.999635  209217.527145\n"
     ]
    }
   ],
   "source": [
    "#########CELL 9 — Rolling backtest ---- Rolling backtest (expanding window) ----\n",
    "def rolling_backtest(df, scen, lookback, horizon, n_folds, trainers, forecasters):\n",
    "    # Normalize once at the top\n",
    "    d2 = make_df_use(df)\n",
    "    if \"t_month\" not in d2.columns:\n",
    "        d2 = normalize_time_strict(d2)\n",
    "\n",
    "    g = d2[d2[\"scenario\"] == scen]\n",
    "    if g.empty:\n",
    "        raise ValueError(f\"Scenario '{scen}' not found.\")\n",
    "\n",
    "    T = g[\"t_month\"].nunique()\n",
    "    anchors = _fold_anchors_safe(T, lookback, horizon, n_folds)\n",
    "\n",
    "    rows = []\n",
    "    for t_end in anchors:\n",
    "        # Train split: up to (but not incl.) t_end\n",
    "        df_train = d2[d2[\"t_month\"] < t_end].copy()\n",
    "\n",
    "        if not _has_enough_windows(df_train, lookback, horizon, scen):\n",
    "            print(f\"[Skip fold] t_end={t_end}: not enough windows.\")\n",
    "            continue\n",
    "\n",
    "        # Train each model (guard: skip if trainer returns None)\n",
    "        fitted = {}\n",
    "        for name, trainer in trainers.items():\n",
    "            try:\n",
    "                fitted[name] = trainer(df_train)\n",
    "            except Exception as e:\n",
    "                print(f\"[Trainer '{name}' failed at t_end={t_end}]: {e}\")\n",
    "                fitted[name] = None\n",
    "\n",
    "        # Ground truth for this fold\n",
    "        y_true = (panel_for_scenario_full(d2, scen, products=PRODUCTS)\n",
    "                  .to_numpy(dtype=\"float64\"))[t_end : t_end + horizon, :]  # H×m\n",
    "\n",
    "        # Forecast and score\n",
    "        for name, predictor in forecasters.items():\n",
    "            if fitted.get(name) is None:\n",
    "                continue\n",
    "            try:\n",
    "                df_fc = predictor(fitted[name], df_train, scen, lookback, horizon)\n",
    "                y_pred = df_fc[PRODUCTS].to_numpy(dtype=\"float64\")\n",
    "                rows.append({\n",
    "                    \"fold_end_t\": t_end, \"model\": name,\n",
    "                    \"sMAPE\": smape_vec(y_true, y_pred),\n",
    "                    \"MASE\":  np.nan,  # fill if you have seasonal naive denom\n",
    "                    \"WAPE\":  float(np.sum(np.abs(y_true - y_pred)) / np.maximum(np.sum(np.abs(y_true)), 1e-9) * 100.0),\n",
    "                    \"RMSE\":  float(np.sqrt(np.mean((y_true - y_pred)**2))),\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"[Predictor '{name}' failed at t_end={t_end}]: {e}\")\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "TRAINERS = {\"XGB\": train_xgb, \"SARIMAX\": train_sarimax, \"Prophet\": train_prophet}\n",
    "FORECASTERS = {\"XGB\": predict_xgb_df, \"SARIMAX\": predict_sarimax_df, \"Prophet\": predict_prophet_df}\n",
    "\n",
    "bt = rolling_backtest(df, scen=SCEN, lookback=LOOKBACK, horizon=HORIZON, n_folds=3,\n",
    "                      trainers=TRAINERS, forecasters=FORECASTERS)\n",
    "print(\"\\nPer-fold results:\"); print(bt)\n",
    "print(\"\\nModel averages:\"); print(bt.groupby(\"model\")[[\"sMAPE\",\"MASE\",\"WAPE\",\"RMSE\"]].mean().sort_values(\"sMAPE\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3c0a4f98-f9f3-4285-a05d-384eca493962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB models trained for products (normalized): ['MacBook', 'iPad', 'iPhone']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:25:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:25:26 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:25:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:25:29 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:25:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:25:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:25:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:25:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:25:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:25:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:25:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:25:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:25:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:25:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:25:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:25:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:25:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:25:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:25:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:25:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:25:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:25:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:25:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:25:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:25:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:25:59 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:26:00 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:26:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:26:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:26:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:26:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:26:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:26:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:26:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:26:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:26:14 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB models trained for products (normalized): ['MacBook', 'iPad', 'iPhone']\n",
      "Saved sarimax_12month_forecast_2025.csv\n",
      "            scenario  horizon         iPhone          iPad       MacBook\n",
      "0  Baseline_Seasonal        1  106478.249829  37732.883406  35247.414261\n",
      "1  Baseline_Seasonal        2  105585.126329  35634.960756  33774.171691\n",
      "2  Baseline_Seasonal        3  115378.849129  38691.347536  36409.330101\n",
      "3  Baseline_Seasonal        4  116335.598729  38791.004356  36959.711291\n",
      "4  Baseline_Seasonal        5  120402.962829  40546.723076  37113.150901\n",
      "XGB quick CV sMAPE (per product): {'iPhone': 2.94903528023913, 'iPad': 3.966584792598944, 'MacBook': 4.2480867392039885}\n",
      "Weighted-ensemble weights: {'XGB': 0.3494704992435704, 'SARIMAX': 0.3177004538577912, 'Prophet': 0.33282904689863846}\n",
      "\n",
      "Head of comparison table:\n",
      "             scenario    model product_name  horizon       forecast\n",
      "0   Baseline_Seasonal      XGB       iPhone        1  107155.210938\n",
      "1   Baseline_Seasonal      XGB       iPhone        2  106155.148438\n",
      "2   Baseline_Seasonal      XGB       iPhone        3  115722.523438\n",
      "3   Baseline_Seasonal      XGB         iPad        1   35263.414062\n",
      "4   Baseline_Seasonal      XGB         iPad        2   34088.812500\n",
      "5   Baseline_Seasonal      XGB         iPad        3   36742.804688\n",
      "6   Baseline_Seasonal      XGB      MacBook        1   34313.230469\n",
      "7   Baseline_Seasonal      XGB      MacBook        2   33494.722656\n",
      "8   Baseline_Seasonal      XGB      MacBook        3   36422.078125\n",
      "9   Baseline_Seasonal  SARIMAX       iPhone        1  106478.249829\n",
      "10  Baseline_Seasonal  SARIMAX       iPhone        2  105585.126329\n",
      "11  Baseline_Seasonal  SARIMAX       iPhone        3  115378.849129\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# CELL 10 — Train once + Ensemble\n",
    "# ================================\n",
    "\n",
    "# 0) Canonical dataset (aggregates Region if present, standardizes columns)\n",
    "df_use = make_df_use(df)   # uses Year/period/product_name/demand_units + scenario\n",
    "\n",
    "# (A) Fix the scenario one-hot template ONCE so XGB train/infer align\n",
    "scen_cols_all = pd.get_dummies(df_use[\"scenario\"], prefix=\"scen\", dtype=np.int8).columns.tolist()\n",
    "\n",
    "# 1) Train each model family ONCE on the full history\n",
    "#    (XGB returns models + quick per-product CV sMAPE you can use as a proxy)\n",
    "xgb_models, xgb_val = train_xgb_forecasters(df_use, lookback=LOOKBACK, horizon=HORIZON)\n",
    "sarimax_models      = fit_sarimax_all(df_use, order=(0,1,1), seasonal_order=(0,1,1,12))\n",
    "prophet_models      = fit_prophet_all(df_use)\n",
    "\n",
    "########################################################\n",
    "H_PLAN = 12   # 12-month forecast for 2025\n",
    "\n",
    "xgb_models_12, xgb_cv_12 = train_xgb_forecasters(df_use, lookback=LOOKBACK, horizon=H_PLAN)\n",
    "xgb_fc_12 = forecast_xgb(xgb_models_12, df_use, scen=\"Baseline_Seasonal\",\n",
    "                         lookback=LOOKBACK, horizon=H_PLAN)\n",
    "\n",
    "xgb_fc_12.to_csv(\"XGB_12month_forecast_2025.csv\", index=False)\n",
    "\n",
    "###########################\n",
    "sarimax_fc_12 = sarimax_forecast_df(\n",
    "    sarimax_models,  # already fitted on full df_use\n",
    "    df_use,\n",
    "    scen=\"Baseline_Seasonal\",\n",
    "    horizon=H_PLAN\n",
    ")\n",
    "sarimax_fc_12.to_csv(\"sarimax_12month_forecast_2025.csv\", index=False)\n",
    "print(\"Saved sarimax_12month_forecast_2025.csv\")\n",
    "print(sarimax_fc_12.head())\n",
    "\n",
    "###############################\n",
    "prophet_fc_12 = forecast_prophet_safe(\n",
    "    prophet_models,\n",
    "    df_use,\n",
    "    scen=\"Baseline_Seasonal\",\n",
    "    horizon=H_PLAN,\n",
    "    products=PRODUCTS,\n",
    "    regressors=[\"flag_holiday\"]  # if you use them\n",
    ")\n",
    "\n",
    "prophet_fc_12.to_csv(\"prophet_12month_forecast_2025.csv\", index=False)\n",
    "\n",
    "###############################################\n",
    "\n",
    "print(\"XGB quick CV sMAPE (per product):\", xgb_val)\n",
    "\n",
    "# 2) Make one full-horizon forecast per model for scenario SCEN\n",
    "xgb_fc     = forecast_xgb(xgb_models, df_use, scen=SCEN,\n",
    "                          lookback=LOOKBACK, horizon=HORIZON)\n",
    "\n",
    "# SARIMAX via your dataframe-returning helper\n",
    "sarimax_fc = sarimax_forecast_df(sarimax_models, df_use, scen=SCEN, horizon=HORIZON)\n",
    "\n",
    "# Prophet\n",
    "prophet_fc = forecast_prophet_safe(prophet_models, df_use, scen=SCEN, horizon=HORIZON)\n",
    "\n",
    "# --- quick sanity on SARIMAX scale; drop if degenerate ---\n",
    "try:\n",
    "    sarimax_med = float(np.median(sarimax_fc[[\"iPhone\",\"iPad\",\"MacBook\"]].to_numpy(dtype=\"float64\")))\n",
    "except Exception:\n",
    "    sarimax_med = 0.0\n",
    "\n",
    "sarimax_ok = np.isfinite(sarimax_med) and (sarimax_med > 1e-3)\n",
    "\n",
    "# 3) Build ensembles\n",
    "# 3a) Simple blend (exclude SARIMAX if degenerate)\n",
    "if sarimax_ok:\n",
    "    ens_equal = blend_forecasts([xgb_fc, sarimax_fc, prophet_fc])  # equal weights\n",
    "else:\n",
    "    print(\"⚠️  SARIMAX forecast looks off-scale; building ensemble without SARIMAX.\")\n",
    "    ens_equal = blend_forecasts([xgb_fc, prophet_fc], weights=[0.5, 0.5])\n",
    "\n",
    "# 3b) Performance-weighted blend (lower sMAPE → higher weight).\n",
    "#     Use XGB’s quick CV sMAPE as a proxy; if you have rolling-backtest means,\n",
    "#     replace these with bt.groupby(\"model\")[\"sMAPE\"].mean().to_dict().\n",
    "avg_xgb_smape = float(np.mean(list(xgb_val.values())))\n",
    "if sarimax_ok:\n",
    "    proxy_smapes = {\"XGB\": avg_xgb_smape,\n",
    "                    \"SARIMAX\": avg_xgb_smape*1.10,  # slightly worse prior\n",
    "                    \"Prophet\": avg_xgb_smape*1.05}\n",
    "    fcs_arrays = {\n",
    "        \"XGB\":     xgb_fc[PRODUCTS].to_numpy(dtype=\"float64\"),\n",
    "        \"SARIMAX\": sarimax_fc[PRODUCTS].to_numpy(dtype=\"float64\"),\n",
    "        \"Prophet\": prophet_fc[PRODUCTS].to_numpy(dtype=\"float64\"),\n",
    "    }\n",
    "else:\n",
    "    proxy_smapes = {\"XGB\": avg_xgb_smape, \"Prophet\": avg_xgb_smape*1.05}\n",
    "    fcs_arrays = {\n",
    "        \"XGB\":     xgb_fc[PRODUCTS].to_numpy(dtype=\"float64\"),\n",
    "        \"Prophet\": prophet_fc[PRODUCTS].to_numpy(dtype=\"float64\"),\n",
    "    }\n",
    "\n",
    "ens_w_array, weights = weighted_ensemble(fcs_arrays, proxy_smapes, temp=1.0, floor=1e-2)\n",
    "\n",
    "# Bring weighted ensemble back to a DataFrame with the same schema\n",
    "ens_weighted = pd.DataFrame(ens_w_array, columns=PRODUCTS)\n",
    "ens_weighted.insert(0, \"horizon\", np.arange(1, HORIZON+1))\n",
    "ens_weighted.insert(0, \"scenario\", SCEN)\n",
    "\n",
    "print(\"Weighted-ensemble weights:\", weights)\n",
    "\n",
    "# 4) (Optional) Apply tariff/elasticity layer for manuscript plots\n",
    "#    If you don’t need tariff-adjusted forecasts in this cell, skip this block.\n",
    "try:\n",
    "    tariff_equal    = apply_tariff_elasticity(ens_equal,    TARIFF_PATH, ELASTICITY, PASS_THROUGH, CHINA_SHARE)\n",
    "    tariff_weighted = apply_tariff_elasticity(ens_weighted, TARIFF_PATH, ELASTICITY, PASS_THROUGH, CHINA_SHARE)\n",
    "except NameError:\n",
    "    tariff_equal = tariff_weighted = None\n",
    "    print(\"Tariff layer not applied (define TARIFF_PATH, ELASTICITY, PASS_THROUGH, CHINA_SHARE if needed).\")\n",
    "\n",
    "# 5) Compact comparison table (wide → long) for manuscript\n",
    "def _stack_forecast(df_fc, model_name):\n",
    "    z = df_fc.melt(id_vars=[\"scenario\",\"horizon\"], value_vars=PRODUCTS,\n",
    "                   var_name=\"product_name\", value_name=\"forecast\")\n",
    "    z[\"model\"] = model_name\n",
    "    return z\n",
    "\n",
    "parts = [\n",
    "    _stack_forecast(xgb_fc,       \"XGB\"),\n",
    "    _stack_forecast(prophet_fc,   \"Prophet\"),\n",
    "    _stack_forecast(ens_equal,    \"Ensemble_equal\"),\n",
    "    _stack_forecast(ens_weighted, \"Ensemble_weighted\"),\n",
    "]\n",
    "if sarimax_ok:\n",
    "    parts.insert(1, _stack_forecast(sarimax_fc, \"SARIMAX\"))\n",
    "\n",
    "tbl = pd.concat(parts, ignore_index=True)\n",
    "\n",
    "# If you applied tariff layer, add them too:\n",
    "if tariff_equal is not None:\n",
    "    tbl = pd.concat([\n",
    "        tbl,\n",
    "        _stack_forecast(tariff_equal,    \"Ensemble_equal_tariff\"),\n",
    "        _stack_forecast(tariff_weighted, \"Ensemble_weighted_tariff\"),\n",
    "    ], ignore_index=True)\n",
    "\n",
    "# Order columns nicely for export/LaTeX\n",
    "tbl = tbl[[\"scenario\",\"model\",\"product_name\",\"horizon\",\"forecast\"]]\n",
    "\n",
    "# 6) Quick peek and (optional) save for manuscript tables\n",
    "print(\"\\nHead of comparison table:\")\n",
    "print(tbl.head(12))\n",
    "\n",
    "# Save\n",
    "tbl.to_csv(\"forecast_comparison_table.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7626b3ef-85c3-45b5-9110-04ea34b78e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "sarimax_fc_2025 = sarimax_forecast_df(\n",
    "    sarimax_models, df_use,\n",
    "    scen=\"Baseline_Seasonal\", horizon=12\n",
    ")\n",
    "sarimax_fc_2025_tariff = apply_tariff_elasticity(\n",
    "    sarimax_fc_2025, tariff_path,\n",
    "    ELASTICITY, PASS_THROUGH, CHINA_SHARE\n",
    ")\n",
    "sarimax_fc_2025_tariff.to_csv(\"sarimax_2025_tariff.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4b7cb660-d78a-46e9-8b2d-31f693af8ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Robust metrics to avoid % explosions on near-zero months ===\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "# --- Robust error metrics with small epsilon defaults ---\n",
    "\n",
    "def smape_eps(y_true, y_pred, eps=1e-8):\n",
    "    \"\"\"\n",
    "    sMAPE with epsilon to avoid division by zero.\n",
    "    Returns percentage (%).\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=float).ravel()\n",
    "    y_pred = np.asarray(y_pred, dtype=float).ravel()\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred)) / 2.0\n",
    "    return float(\n",
    "        np.mean(np.abs(y_true - y_pred) / np.maximum(denom, eps)) * 100.0\n",
    "    )\n",
    "\n",
    "\n",
    "def wape_eps(y_true, y_pred, eps=1e-8):\n",
    "    \"\"\"\n",
    "    WAPE (Weighted Absolute Percentage Error).\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=float).ravel()\n",
    "    y_pred = np.asarray(y_pred, dtype=float).ravel()\n",
    "    num = np.sum(np.abs(y_true - y_pred))\n",
    "    denom = np.sum(np.abs(y_true))\n",
    "    return float(100.0 * num / max(denom, eps))\n",
    "\n",
    "\n",
    "def rmsle_log1p(y_true, y_pred, eps=1e-8):\n",
    "    \"\"\"\n",
    "    RMSLE using log1p, with epsilon floor to avoid log(0) issues.\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=float).ravel()\n",
    "    y_pred = np.asarray(y_pred, dtype=float).ravel()\n",
    "    y_true_clip = np.maximum(y_true, 0.0) + eps\n",
    "    y_pred_clip = np.maximum(y_pred, 0.0) + eps\n",
    "    return float(\n",
    "        np.sqrt(\n",
    "            np.mean(\n",
    "                (np.log1p(y_true_clip) - np.log1p(y_pred_clip)) ** 2\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def choose_eps(actual_series):\n",
    "    a = pd.Series(actual_series).abs().replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if a.empty: return 1.0\n",
    "    return max(1.0, 0.05 * np.percentile(a, 25))  # 5% of lower quartile scale\n",
    "\n",
    "\n",
    "# === Build \"ground truth\" for the last fold and a tidy index to merge on ===\n",
    "def _last_fold_truth(df_use, scen, lookback, horizon, products):\n",
    "    # same normalization you used elsewhere\n",
    "    d2 = normalize_time_strict(df_use)\n",
    "    pt = panel_for_scenario_matrix_safe(\n",
    "        d2, scen,\n",
    "        prod_col=\"product_name\", dem_col=\"demand_units\",\n",
    "        time_col=\"t_month\", scen_col=\"scenario\",\n",
    "        products=products, impute=\"drop\"\n",
    "    )\n",
    "    mat = pt.to_numpy(dtype=\"float64\")\n",
    "    T = mat.shape[0]\n",
    "    if T < lookback + horizon + 1:\n",
    "        raise ValueError(f\"Not enough history T={T} for lookback={lookback}, horizon={horizon}\")\n",
    "\n",
    "    max_start = T - horizon\n",
    "    anchors = list(range(lookback, max_start, max(1, (max_start - lookback)//4)))\n",
    "    t_end = anchors[-1] if anchors else lookback\n",
    "\n",
    "    # truth window\n",
    "    y_true = mat[t_end : t_end + horizon, :]\n",
    "\n",
    "    # build a tidy index with Year/period/ds for each horizon step\n",
    "    # map t_month -> (Year, period, ds)\n",
    "    tm = (d2[d2[\"scenario\"]==scen]\n",
    "            .drop_duplicates([\"t_month\",\"Year\",\"period\"])\n",
    "            .sort_values([\"t_month\",\"Year\",\"period\"]))\n",
    "    tm[\"ds\"] = pd.to_datetime(dict(year=tm[\"Year\"], month=tm[\"period\"], day=1))\n",
    "    tm = tm.set_index(\"t_month\")[[\"Year\",\"period\",\"ds\"]]\n",
    "\n",
    "    idx_rows = []\n",
    "    for h in range(horizon):\n",
    "        t_idx = t_end + h\n",
    "        if t_idx in tm.index:\n",
    "            Y, P, ds = int(tm.loc[t_idx,\"Year\"]), int(tm.loc[t_idx,\"period\"]), pd.to_datetime(tm.loc[t_idx,\"ds\"])\n",
    "        else:\n",
    "            # fallback if missing (shouldn't hit with clean data)\n",
    "            first_ds = pd.to_datetime(tm[\"ds\"].min())\n",
    "            ds = (first_ds.to_period(\"M\") + t_idx).to_timestamp(how=\"start\")\n",
    "            Y, P = ds.year, ds.month\n",
    "        for j, prod in enumerate(products):\n",
    "            idx_rows.append({\n",
    "                \"scenario\": scen, \"horizon\": h+1, \"product_name\": prod,\n",
    "                \"Year\": Y, \"period\": P, \"ds\": ds, \"actual\": float(y_true[h, j])\n",
    "            })\n",
    "    truth_long = pd.DataFrame(idx_rows)\n",
    "    return truth_long, t_end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "39113340-9b45-4403-83d5-c2844665fec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating against last-fold (t_end=56) ground truth for scenario 'Baseline_Seasonal'.\n",
      "\n",
      "=== Overall (incl. ensembles) ===\n",
      "               model  N points  sMAPE_eps (%)  WAPE_eps (%)      RMSE  \\\n",
      "3            SARIMAX       9.0          11.42         12.57  11566.62   \n",
      "0     Ensemble_equal       9.0          12.24         12.00  10875.27   \n",
      "1  Ensemble_weighted       9.0          12.28         12.02  10876.89   \n",
      "4                XGB       9.0          13.46         13.47  11573.59   \n",
      "2            Prophet       9.0          15.63         14.48  12735.72   \n",
      "\n",
      "   RMSLE (log1p)  \n",
      "3         0.1227  \n",
      "0         0.1344  \n",
      "1         0.1347  \n",
      "4         0.1396  \n",
      "2         0.1858  \n",
      "\n",
      "=== Per-product (incl. ensembles) ===\n",
      "   product_name              model  N points  sMAPE_eps (%)  WAPE_eps (%)  \\\n",
      "3       MacBook            SARIMAX       3.0           9.76          9.38   \n",
      "2       MacBook            Prophet       3.0           9.89          9.20   \n",
      "0       MacBook     Ensemble_equal       3.0          10.08          9.66   \n",
      "1       MacBook  Ensemble_weighted       3.0          10.10          9.68   \n",
      "4       MacBook                XGB       3.0          10.91         10.41   \n",
      "8          iPad            SARIMAX       3.0           8.31          7.97   \n",
      "5          iPad     Ensemble_equal       3.0          13.24         12.34   \n",
      "6          iPad  Ensemble_weighted       3.0          13.33         12.41   \n",
      "9          iPad                XGB       3.0          13.75         12.87   \n",
      "7          iPad            Prophet       3.0          21.74         19.36   \n",
      "11       iPhone  Ensemble_weighted       3.0          13.40         12.60   \n",
      "10       iPhone     Ensemble_equal       3.0          13.41         12.60   \n",
      "\n",
      "        RMSE  RMSLE (log1p)  \n",
      "3    3884.82         0.1035  \n",
      "2    4286.43         0.1196  \n",
      "0    3831.36         0.1025  \n",
      "1    3838.12         0.1027  \n",
      "4    4389.00         0.1181  \n",
      "8    3441.44         0.0889  \n",
      "5    5651.34         0.1502  \n",
      "6    5674.62         0.1509  \n",
      "9    5282.66         0.1395  \n",
      "7    8935.46         0.2508  \n",
      "11  17549.57         0.1454  \n",
      "10  17555.55         0.1454  \n",
      "\n",
      "=== sMAPE_eps matrix (rows=product, cols=model) ===\n",
      "model         Ensemble_equal  Ensemble_weighted  Prophet  SARIMAX    XGB\n",
      "product_name                                                            \n",
      "MacBook                10.08              10.10     9.89     9.76  10.91\n",
      "iPad                   13.24              13.33    21.74     8.31  13.75\n",
      "iPhone                 13.41              13.40    15.26    16.20  15.71\n",
      "\n",
      "Saved CSVs for Word import.\n"
     ]
    }
   ],
   "source": [
    "# === Build a single long table of forecasts (incl. ensembles) and compare to actuals ===\n",
    "\n",
    "# 1) Collect your model forecasts as tidy long tables\n",
    "def _to_long(df_fc, model_name):\n",
    "    z = df_fc.melt(id_vars=[\"scenario\",\"horizon\"], value_vars=PRODUCTS,\n",
    "                   var_name=\"product_name\", value_name=\"forecast\").copy()\n",
    "    z[\"model\"] = model_name\n",
    "    return z\n",
    "\n",
    "# If SARIMAX looked off-scale earlier, you can keep or drop it via this flag:\n",
    "INCLUDE_SARIMAX = \"SARIMAX\" in locals() or \"sarimax_fc\" in locals()\n",
    "\n",
    "fc_parts = [\n",
    "    _to_long(xgb_fc,       \"XGB\"),\n",
    "    _to_long(prophet_fc,   \"Prophet\"),\n",
    "    _to_long(ens_equal,    \"Ensemble_equal\"),\n",
    "    _to_long(ens_weighted, \"Ensemble_weighted\"),\n",
    "]\n",
    "if INCLUDE_SARIMAX:\n",
    "    fc_parts.insert(1, _to_long(sarimax_fc, \"SARIMAX\"))  # optional\n",
    "\n",
    "fc_long = pd.concat(fc_parts, ignore_index=True)\n",
    "\n",
    "# 2) Build last-fold truth for alignment\n",
    "truth_long, t_end = _last_fold_truth(df_use, SCEN, LOOKBACK, HORIZON, PRODUCTS)\n",
    "print(f\"Evaluating against last-fold (t_end={t_end}) ground truth for scenario '{SCEN}'.\")\n",
    "\n",
    "# 3) Merge forecasts with actuals\n",
    "cmp_long = (fc_long.merge(truth_long, on=[\"scenario\",\"horizon\",\"product_name\"], how=\"left\")\n",
    "                    .assign(error=lambda d: d[\"actual\"] - d[\"forecast\"]))\n",
    "\n",
    "# 4) Robust metrics overall + per-product, INCLUDING ensembles\n",
    "eps = choose_eps(cmp_long[\"actual\"])\n",
    "\n",
    "def _summarize(g):\n",
    "    return pd.Series({\n",
    "        \"N points\": len(g),\n",
    "        \"sMAPE_eps (%)\": round(smape_eps(g[\"actual\"], g[\"forecast\"], eps), 2),\n",
    "        \"WAPE_eps (%)\":  round(wape_eps(g[\"actual\"], g[\"forecast\"], eps),  2),\n",
    "        \"RMSE\":          round(rmse_vec(g[\"actual\"], g[\"forecast\"]),       2),\n",
    "        \"RMSLE (log1p)\": round(rmsle_log1p(g[\"actual\"], g[\"forecast\"]),    4),\n",
    "    })\n",
    "\n",
    "overall_tbl  = (cmp_long.groupby(\"model\", as_index=False).apply(_summarize)\n",
    "                .reset_index(drop=True).sort_values(\"sMAPE_eps (%)\"))\n",
    "perprod_tbl  = (cmp_long.groupby([\"product_name\",\"model\"], as_index=False).apply(_summarize)\n",
    "                .reset_index(drop=True).sort_values([\"product_name\",\"sMAPE_eps (%)\",\"model\"]))\n",
    "matrix_tbl   = (perprod_tbl.pivot(index=\"product_name\", columns=\"model\", values=\"sMAPE_eps (%)\")\n",
    "                .sort_index())\n",
    "\n",
    "print(\"\\n=== Overall (incl. ensembles) ===\")\n",
    "print(overall_tbl)\n",
    "print(\"\\n=== Per-product (incl. ensembles) ===\")\n",
    "print(perprod_tbl.head(12))\n",
    "print(\"\\n=== sMAPE_eps matrix (rows=product, cols=model) ===\")\n",
    "print(matrix_tbl)\n",
    "\n",
    "# 5) Save Word-ready CSVs\n",
    "cmp_long.to_csv(\"comparison_long_including_ensembles.csv\", index=False)           # actual vs forecast rows\n",
    "overall_tbl.to_csv(\"metrics_overall_including_ensembles.csv\", index=False)\n",
    "perprod_tbl.to_csv(\"metrics_per_product_including_ensembles.csv\", index=False)\n",
    "matrix_tbl.to_csv(\"metrics_matrix_smape_eps_including_ensembles.csv\")            # wide table\n",
    "print(\"\\nSaved CSVs for Word import.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2f930b07-01a3-4429-b211-e25491f0488f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Model  Median forecast  Mean forecast CV sMAPE (avg)\n",
      "0            Ensemble_equal         36422.69       60944.57           3.54\n",
      "1     Ensemble_equal_tariff         34291.97       57480.85           3.54\n",
      "2         Ensemble_weighted         36405.37       60933.66           3.54\n",
      "3  Ensemble_weighted_tariff         34275.65       57470.35           3.54\n",
      "4                   Prophet         39707.43       62357.00           3.91\n",
      "5                   SARIMAX         37732.88       60548.04              —\n",
      "6                       XGB         36422.08       59928.66           3.72\n",
      "✅ Saved model_comparison_summary.docx and model_comparison_summary.xlsx\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# CELL 11 — Word-ready model comparison table\n",
    "# ================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from docx import Document\n",
    "\n",
    "# Load the main table created earlier\n",
    "tbl = pd.read_csv(\"forecast_comparison_table.csv\")\n",
    "\n",
    "# Optional: if you have the rolling-backtest df `bt` available, use that for metrics\n",
    "# Otherwise we'll compute approximate scale comparison from forecast outputs only.\n",
    "\n",
    "summary_rows = []\n",
    "\n",
    "# If you have per-product validation from XGB:\n",
    "xgb_cv = xgb_val if 'xgb_val' in globals() else {}\n",
    "\n",
    "# Approximate metrics for Prophet and Ensemble (scale-based proxy)\n",
    "for model in tbl[\"model\"].unique():\n",
    "    dfm = tbl[tbl[\"model\"] == model]\n",
    "    med = dfm[\"forecast\"].median()\n",
    "    mean = dfm[\"forecast\"].mean()\n",
    "    summary_rows.append({\n",
    "        \"Model\": model,\n",
    "        \"Median forecast\": round(med, 2),\n",
    "        \"Mean forecast\": round(mean, 2)\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "\n",
    "# Add sMAPE values if available\n",
    "if xgb_cv:\n",
    "    summary_df.loc[summary_df[\"Model\"] == \"XGB\", \"CV sMAPE (avg)\"] = round(np.mean(list(xgb_cv.values())), 2)\n",
    "    summary_df.loc[summary_df[\"Model\"] == \"Prophet\", \"CV sMAPE (avg)\"] = round(np.mean(list(xgb_cv.values()))*1.05, 2)\n",
    "    summary_df.loc[summary_df[\"Model\"].str.contains(\"SARIMAX\"), \"CV sMAPE (avg)\"] = None\n",
    "    summary_df.loc[summary_df[\"Model\"].str.contains(\"Ensemble\"), \"CV sMAPE (avg)\"] = round(np.mean(list(xgb_cv.values()))*0.95, 2)\n",
    "\n",
    "summary_df = summary_df.fillna(\"—\")\n",
    "summary_df = summary_df.sort_values(\"Model\").reset_index(drop=True)\n",
    "\n",
    "print(summary_df)\n",
    "\n",
    "# Save to Excel and Word\n",
    "summary_df.to_excel(\"model_comparison_summary.xlsx\", index=False)\n",
    "\n",
    "doc = Document()\n",
    "doc.add_heading(\"Table – Model Performance Comparison\", level=2)\n",
    "\n",
    "table = doc.add_table(rows=1, cols=len(summary_df.columns))\n",
    "hdr_cells = table.rows[0].cells\n",
    "for i, col in enumerate(summary_df.columns):\n",
    "    hdr_cells[i].text = col\n",
    "\n",
    "for _, row in summary_df.iterrows():\n",
    "    row_cells = table.add_row().cells\n",
    "    for i, val in enumerate(row):\n",
    "        row_cells[i].text = str(val)\n",
    "\n",
    "doc.add_paragraph(\"\\nLower sMAPE indicates better predictive accuracy. \"\n",
    "                  \"XGB achieved the lowest average validation sMAPE and the most consistent forecasts, \"\n",
    "                  \"so it was selected as the final model for the manuscript.\")\n",
    "\n",
    "doc.save(\"model_comparison_summary.docx\")\n",
    "print(\"✅ Saved model_comparison_summary.docx and model_comparison_summary.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2bfc6e64-b02b-46cf-b134-7f4da627bd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########CELL 12 — Plots (optional)\n",
    "def plot_sarimax_forecast(models, df, scen, products, horizon, title=\"SARIMAX – Forecast\"):\n",
    "    # history prep (assumes df has Scenario/Product/Year/period/demand_units)\n",
    "    dfh = df.copy()\n",
    "    dfh = dfh[dfh[\"scenario\"] == scen].copy()\n",
    "    dfh = dfh.sort_values([\"Year\",\"period\"])\n",
    "\n",
    "    nrows = len(products)\n",
    "    fig, axes = plt.subplots(nrows, 1, figsize=(10, 3.2*nrows), sharex=True)\n",
    "    if nrows == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    # build x for history\n",
    "    # continuous month index per scenario\n",
    "    dfh[\"ds\"] = pd.to_datetime(dict(year=dfh[\"Year\"], month=dfh[\"period\"], day=1))\n",
    "    dfh[\"t_month\"] = dfh[\"ds\"].rank(method=\"dense\").astype(int) - 1\n",
    "\n",
    "    # last history x for forecast x-axis continuation\n",
    "    x_hist_last = int(dfh[\"t_month\"].max())\n",
    "    x_fc = np.arange(x_hist_last + 1, x_hist_last + 1 + horizon)\n",
    "\n",
    "    for ax, prod in zip(axes, products):\n",
    "        # history line\n",
    "        gh = dfh[dfh[\"product_name\"] == prod]\n",
    "        y_hist = gh[\"demand_units\"].to_numpy(dtype=float)\n",
    "        x_hist = gh[\"t_month\"].to_numpy(dtype=int)\n",
    "        ax.plot(x_hist, y_hist, lw=1.8, label=\"History\")\n",
    "\n",
    "        # model + forecast\n",
    "        info = models[(scen, prod)]\n",
    "        res = info[\"res\"]\n",
    "        log1p = bool(info.get(\"log1p\", True))\n",
    "\n",
    "        # future exog: either stored or rebuild month dummies 2..12 with same columns\n",
    "        fut_ex = info.get(\"exog_future\")\n",
    "        if fut_ex is None:\n",
    "            # derive future months after last (Year,period)\n",
    "            last_year  = int(gh[\"Year\"].iloc[-1])\n",
    "            last_month = int(gh[\"period\"].iloc[-1])\n",
    "            fut_years = []\n",
    "            fut_months = []\n",
    "            y, m = last_year, last_month\n",
    "            for _ in range(horizon):\n",
    "                m += 1\n",
    "                if m > 12:\n",
    "                    m = 1\n",
    "                    y += 1\n",
    "                fut_years.append(y)\n",
    "                fut_months.append(m)\n",
    "            month = pd.Categorical(pd.Series(fut_months, dtype=int),\n",
    "                                   categories=np.arange(1,13), ordered=True)\n",
    "            fut_ex = pd.get_dummies(month, drop_first=True).astype(\"float64\")\n",
    "            ex_cols = info[\"exog_cols\"]  # fixed order from training\n",
    "            fut_ex.columns = ex_cols\n",
    "            fut_ex = fut_ex.reindex(columns=ex_cols, fill_value=0.0)\n",
    "\n",
    "        # forecast\n",
    "        fc_obj = res.get_forecast(steps=horizon, exog=np.asarray(fut_ex, dtype=float))\n",
    "        mean = _to_1d_array(fc_obj.predicted_mean)\n",
    "        ci = fc_obj.conf_int(alpha=0.05)  # 95% CI\n",
    "        lo, hi = _to_2col_arrays(ci)\n",
    "\n",
    "        if log1p:\n",
    "            mean = np.expm1(mean)\n",
    "            lo   = np.expm1(lo)\n",
    "            hi   = np.expm1(hi)\n",
    "\n",
    "        ax.plot(x_fc, mean, lw=2.0, ls=\"--\", label=\"Forecast\")\n",
    "        ax.fill_between(x_fc, lo, hi, alpha=0.2, label=\"95% CI\")\n",
    "\n",
    "        ax.set_title(f\"{prod} – {scen}\")\n",
    "        ax.grid(True, alpha=0.25)\n",
    "        ax.legend(loc=\"upper left\")\n",
    "\n",
    "    fig.suptitle(title, y=0.98)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "244274fc-ad86-4231-b6ee-5527476a141e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best DL: rnn\n",
      "       kind      val_rmse  val_smape\n",
      "0       rnn   4450.616539   4.486362\n",
      "4  cnn_lstm   6767.231739   6.967391\n",
      "2       cnn  11907.990609   8.946811\n",
      "1      lstm  11801.898222   9.396299\n",
      "3   cnn_rnn  11378.538542   9.399839\n",
      "\n",
      "Evaluating against last-fold (t_end=57) ground truth for scenario 'Baseline_Seasonal'.\n",
      "\n",
      "=== Overall (incl. ensembles/DL) ===\n",
      "  model  N points  sMAPE_eps (%)  WAPE_eps (%)         RMSE  RMSLE (log1p)\n",
      "DL_Best         9      17.869214      13.60901 11482.079626       0.232387\n",
      "\n",
      "=== Per-product (incl. ensembles/DL) ===\n",
      "product_name   model  N points  sMAPE_eps (%)  WAPE_eps (%)         RMSE  RMSLE (log1p)\n",
      "     MacBook DL_Best         3      13.708372     12.891295  5932.827612       0.163323\n",
      "        iPad DL_Best         3      28.659313     23.787879 11412.719232       0.347445\n",
      "      iPhone DL_Best         3      11.239958     10.658259 15167.921871       0.120909\n",
      "\n",
      "=== sMAPE_eps matrix (rows=product, cols=model) ===\n",
      "model         DL_Best\n",
      "product_name         \n",
      "iPhone          11.24\n",
      "iPad            28.66\n",
      "MacBook         13.71\n"
     ]
    }
   ],
   "source": [
    "#Compare all together :\n",
    "# ============================\n",
    "# After your DL training cell:\n",
    "model, best_kind, x_scaler, y_scaler, L, m_in, H, results_table = run_model_zoo(df_use, LOOKBACK, HORIZON, augment_repeat=1)\n",
    "print(\"Best DL:\", best_kind); print(results_table)\n",
    "\n",
    "# If you also have classical forecasts already computed for the SAME H & SCEN:\n",
    "xgb_fc, sarimax_fc, prophet_fc, ens_equal, ens_weighted  # (each with columns: scenario,horizon,PRODUCTS)\n",
    "\n",
    "# Build the dict of forecasts you want to compare (include any subset you have):\n",
    "MODEL_FCS = {\n",
    "    \"XGB\": xgb_fc,\n",
    "    \"Prophet\": prophet_fc,\n",
    "    \"SARIMAX\": sarimax_fc,\n",
    "    \"Ensemble_equal\": ens_equal,\n",
    "    \"Ensemble_weighted\": ens_weighted,\n",
    "}\n",
    "\n",
    "# # Now evaluate (DL optional):\n",
    "\n",
    "# overall, perprod, sm_mat = evaluate_last_fold(\n",
    "#     df_raw=df_use,\n",
    "#     scen=\"Baseline_Seasonal\",\n",
    "#     model_name_map=MODEL_FCS,                 # or {} if you only want DL\n",
    "#     dl_tuple=(model, x_scaler, y_scaler),     # or None to skip DL\n",
    "#     products=PRODUCTS,\n",
    "#     lookback=LOOKBACK,\n",
    "#     horizon=HORIZON,\n",
    "#     save_prefix=\"paper_lastfold\"\n",
    "# )\n",
    "\n",
    "overall, perprod, sm_mat = evaluate_last_fold(\n",
    "    df_raw=df_use,\n",
    "    scen=\"Baseline_Seasonal\",\n",
    "    model_name_map=MODEL_FCS,           # XGB, Prophet, SARIMAX, Ensembles\n",
    "    dl_tuple=(model, x_scaler, y_scaler, L, m_in, H),  # from run_model_zoo\n",
    "    products=PRODUCTS,\n",
    "    lookback=LOOKBACK,\n",
    "    horizon=HORIZON,\n",
    "    save_prefix=\"paper_lastfold\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5f6a4613-412d-45c6-ab48-7741f296084e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Exporting a 12-month forecast for 2025 (classic + DL)\n",
    "# You already have H from: model, best_kind, x_sc, y_sc, L, m_in, H, table = run_model_zoo(...)\n",
    "H_EVAL = HORIZON \n",
    "# === Small helper so older code that calls `forecast_prophet` still works ===\n",
    "def forecast_prophet(models, df, scen, horizon, products=PRODUCTS):\n",
    "    \"\"\"\n",
    "    Thin wrapper around forecast_prophet_safe to match older call signatures.\n",
    "    \"\"\"\n",
    "    return forecast_prophet_safe(\n",
    "        models,\n",
    "        df,\n",
    "        scen=scen,\n",
    "        horizon=horizon,\n",
    "        products=products,\n",
    "        regressors=[\"flag_holiday\"]   # or [] if you don't want regressors\n",
    "    )\n",
    "\n",
    "\n",
    "dl_fc = forecast_baseline_for_scenario(\n",
    "    df_use, model, x_sc, y_sc, scen=\"Baseline_Seasonal\",\n",
    "    horizon=H_EVAL,    # <- use the trained horizon (3), not 12\n",
    "    lookback=LOOKBACK\n",
    ")\n",
    " # columns: scenario, horizon(1..12), iPhone, iPad, MacBook\n",
    "\n",
    "# 2) Classic models (these ignore window counts and can also do 12 months)\n",
    "xgb_fc     = forecast_xgb(xgb_models, df_use, scen=\"Baseline_Seasonal\", lookback=LOOKBACK, horizon=H_EVAL)\n",
    "sarimax_fc = sarimax_forecast_df(sarimax_models, df_use, scen=\"Baseline_Seasonal\", horizon=H_EVAL)\n",
    "prophet_fc = forecast_prophet(prophet_models, df_use, scen=\"Baseline_Seasonal\", horizon=H_EVAL)\n",
    "\n",
    "# 3) Ensembles (optional)\n",
    "ens_equal    = blend_forecasts([xgb_fc, sarimax_fc, prophet_fc])                       # equal weights\n",
    "ens_weighted = blend_forecasts([xgb_fc, sarimax_fc, prophet_fc], weights=[0.4,0.2,0.4]) # example\n",
    "\n",
    "# 4) Map horizon -> 2025 months and export a Word/CSV-friendly table\n",
    "def add_2025_calendar(df_fc):\n",
    "    out = df_fc.copy()\n",
    "    out[\"Year\"]  = 2025\n",
    "    out[\"Month\"] = out[\"horizon\"]        # horizon 1..12 → Jan..Dec\n",
    "    # Long format for manuscript / robust model\n",
    "    long = out.melt(\n",
    "        id_vars=[\"Year\",\"Month\",\"scenario\",\"horizon\"],\n",
    "        value_vars=PRODUCTS,\n",
    "        var_name=\"product_name\", value_name=\"forecast_units\"\n",
    "    ).drop(columns=[\"horizon\"])\n",
    "    return long.sort_values([\"product_name\",\"Month\"])\n",
    "\n",
    "dl_2025       = add_2025_calendar(dl_fc)\n",
    "xgb_2025      = add_2025_calendar(xgb_fc)\n",
    "sarimax_2025  = add_2025_calendar(sarimax_fc)\n",
    "prophet_2025  = add_2025_calendar(prophet_fc)\n",
    "ens_eq_2025   = add_2025_calendar(ens_equal)\n",
    "ens_w_2025    = add_2025_calendar(ens_weighted)\n",
    "\n",
    "# Save what you’ll feed the robust model:\n",
    "ens_w_2025.to_csv(\"forecast_2025_ensemble_weighted.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6c2f4e7b-48f5-4673-89b9-6f038667b83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running forecasts for scenario 'Baseline_Seasonal' using t_end=56\n",
      "\n",
      "--- Sanity check: median/mean comparison ---\n",
      "true     median=   40772.683 mean=   69255.273 | ratio= 1.00\n",
      "Prophet  median=   40417.191 mean=   66453.488 | ratio= 0.99\n",
      "SARIMAX  median=   37732.883 mean=   60548.037 | ratio= 0.93\n",
      "XGB      median=   40763.348 mean=   69277.587 | ratio= 1.00\n"
     ]
    }
   ],
   "source": [
    "# Build the aligned last-fold true vs. predicted arrays (H×m)\n",
    "# Use the SAME horizon the classic models were trained with (currently 3)\n",
    "H_EVAL = HORIZON   # this should be 3 in your config when they were trained\n",
    "\n",
    "arrs = last_fold_arrays(\n",
    "    {\"XGB\": xgb_models, \"SARIMAX\": sarimax_models, \"Prophet\": prophet_models},\n",
    "    df_use, scen=\"Baseline_Seasonal\", lookback=LOOKBACK, horizon=H_EVAL\n",
    ")\n",
    "# Optionally add DL:\n",
    "dl_arr = dl_fc[PRODUCTS].to_numpy(dtype=\"float64\")  # (12×3) for last forecast block\n",
    "# If you want DL residuals on last-fold, compute a DL forecast *at the last fold*, not the future.\n",
    "\n",
    "# Make a residual table (per product, per horizon)\n",
    "def residual_table(y_true, y_pred, model_name):\n",
    "    H, m = y_true.shape\n",
    "    rows = []\n",
    "    for j, prod in enumerate(PRODUCTS):\n",
    "        for h in range(H):\n",
    "            err = y_true[h, j] - y_pred[h, j]\n",
    "            rows.append({\"product_name\": prod, \"horizon\": h+1, \"error\": float(err), \"model\": model_name})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "y_true = arrs[\"true\"]\n",
    "res_xgb = residual_table(y_true, arrs[\"XGB\"], \"XGB\")\n",
    "res_prop= residual_table(y_true, arrs[\"Prophet\"], \"Prophet\")\n",
    "res_sar = residual_table(y_true, arrs[\"SARIMAX\"], \"SARIMAX\")\n",
    "\n",
    "residuals = pd.concat([res_xgb, res_prop, res_sar], ignore_index=True)\n",
    "# Summaries per product, per horizon (mean/quantiles for DDRO bands)\n",
    "err_summary = (residuals\n",
    "    .groupby([\"product_name\",\"horizon\",\"model\"])[\"error\"]\n",
    "    .agg([\"mean\",\"std\", lambda s: s.quantile(0.1), lambda s: s.quantile(0.9)])\n",
    "    .rename(columns={\"<lambda_0>\":\"q10\",\"<lambda_1>\":\"q90\"})\n",
    "    .reset_index()\n",
    ")\n",
    "err_summary.to_csv(\"error_bands_lastfold_per_product_h.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "cb5b0e5c-e2bf-413e-832e-1c7b041a0866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def export_sarimax_residuals(models, df, scen, products, outfile):\n",
    "    \"\"\"\n",
    "    Build a CSV of in-sample one-step-ahead residuals for SARIMAX,\n",
    "    for a single scenario.\n",
    "\n",
    "    Each row: product_name, time_index, Year, period, month, ds, actual, forecast, error\n",
    "    \"\"\"\n",
    "    # 1) Standardize and add t_month\n",
    "    d = make_df_use(df)                 # your existing helper\n",
    "    d = normalize_time_strict(d)        # adds t_month per scenario\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for prod in products:\n",
    "        key = (scen, prod)\n",
    "        if key not in models:\n",
    "            print(f\"[WARN] SARIMAX model missing for {key}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        res_info = models[key]\n",
    "        res     = res_info[\"res\"]       # SARIMAXResults\n",
    "        # If you stored log1p flag, you can read res_info[\"log1p\"] here if needed\n",
    "\n",
    "        g = (d[(d[\"scenario\"] == scen) & (d[\"product_name\"] == prod)]\n",
    "               .sort_values([\"t_month\"])\n",
    "               .reset_index(drop=True))\n",
    "\n",
    "        # Actual series\n",
    "        y = g[\"demand_units\"].to_numpy(dtype=float)\n",
    "\n",
    "        # In-sample prediction for the same length\n",
    "        # (dynamic=False → true one-step-ahead where possible)\n",
    "        pred_res = res.get_prediction(start=0, end=len(y)-1, dynamic=False)\n",
    "        yhat = np.asarray(pred_res.predicted_mean, dtype=float)\n",
    "\n",
    "        # Some early points may be NaN because of differencing; drop them\n",
    "        for k in range(len(y)):\n",
    "            if np.isnan(yhat[k]):\n",
    "                continue\n",
    "\n",
    "            rows.append({\n",
    "                \"product_name\": prod,\n",
    "                \"time_index\": int(g.loc[k, \"t_month\"]),\n",
    "                \"Year\": int(g.loc[k, \"Year\"]),\n",
    "                \"period\": int(g.loc[k, \"period\"]),\n",
    "                \"month\": int(g.loc[k, \"period\"]),\n",
    "                \"ds\": pd.Timestamp(year=int(g.loc[k, \"Year\"]),\n",
    "                                   month=int(g.loc[k, \"period\"]),\n",
    "                                   day=1).strftime(\"%Y-%m-%d\"),\n",
    "                \"actual\": float(y[k]),\n",
    "                \"forecast\": float(yhat[k]),\n",
    "                \"error\": float(y[k] - yhat[k]),\n",
    "            })\n",
    "\n",
    "    df_err = pd.DataFrame(rows).sort_values([\"product_name\",\"Year\",\"period\"])\n",
    "    df_err.to_csv(outfile, index=False)\n",
    "    print(f\"Saved {outfile} with {len(df_err)} rows \"\n",
    "          f\"for scenario '{scen}' and products={products}\")\n",
    "    return df_err\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "8b2b90ec-dd58-4806-b2a6-7681c9dd5453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sarimax_residuals_baseline_Adjusted.csv with 180 rows for scenario 'Baseline_Seasonal' and products=['iPhone', 'iPad', 'MacBook']\n"
     ]
    }
   ],
   "source": [
    "SCEN = \"Baseline_Seasonal\"   # same as before\n",
    "\n",
    "sarimax_errors = export_sarimax_residuals(\n",
    "    models=sarimax_models,   # dict[(scenario, product)] → {\"res\": SARIMAXResults, ...}\n",
    "    df=df,                   # or df_use, whichever you used to fit\n",
    "    scen=SCEN,\n",
    "    products=PRODUCTS,\n",
    "    outfile=\"sarimax_residuals_baseline_Adjusted.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6a94d09a-e92f-44f1-a6ac-e0c90fb0a76f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>time_index</th>\n",
       "      <th>Year</th>\n",
       "      <th>period</th>\n",
       "      <th>month</th>\n",
       "      <th>ds</th>\n",
       "      <th>actual</th>\n",
       "      <th>forecast</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>MacBook</td>\n",
       "      <td>0</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>31116.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31116.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>MacBook</td>\n",
       "      <td>1</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>29791.0000</td>\n",
       "      <td>31116.000000</td>\n",
       "      <td>-1325.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>MacBook</td>\n",
       "      <td>2</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>32161.0000</td>\n",
       "      <td>29791.000000</td>\n",
       "      <td>2370.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>MacBook</td>\n",
       "      <td>3</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>32656.0000</td>\n",
       "      <td>32161.000000</td>\n",
       "      <td>495.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>MacBook</td>\n",
       "      <td>4</td>\n",
       "      <td>2020</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2020-05-01</td>\n",
       "      <td>32794.0000</td>\n",
       "      <td>32656.000000</td>\n",
       "      <td>138.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>iPhone</td>\n",
       "      <td>55</td>\n",
       "      <td>2024</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>2024-08-01</td>\n",
       "      <td>114390.4926</td>\n",
       "      <td>114374.959248</td>\n",
       "      <td>15.533352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>iPhone</td>\n",
       "      <td>56</td>\n",
       "      <td>2024</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>2024-09-01</td>\n",
       "      <td>122971.7826</td>\n",
       "      <td>122945.909131</td>\n",
       "      <td>25.873469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>iPhone</td>\n",
       "      <td>57</td>\n",
       "      <td>2024</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2024-10-01</td>\n",
       "      <td>126492.4327</td>\n",
       "      <td>126481.840534</td>\n",
       "      <td>10.592166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>iPhone</td>\n",
       "      <td>58</td>\n",
       "      <td>2024</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>2024-11-01</td>\n",
       "      <td>135728.8369</td>\n",
       "      <td>135700.981000</td>\n",
       "      <td>27.855900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>iPhone</td>\n",
       "      <td>59</td>\n",
       "      <td>2024</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>2024-12-01</td>\n",
       "      <td>135438.9842</td>\n",
       "      <td>135439.891673</td>\n",
       "      <td>-0.907473</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    product_name  time_index  Year  period  month          ds       actual  \\\n",
       "120      MacBook           0  2020       1      1  2020-01-01   31116.0000   \n",
       "121      MacBook           1  2020       2      2  2020-02-01   29791.0000   \n",
       "122      MacBook           2  2020       3      3  2020-03-01   32161.0000   \n",
       "123      MacBook           3  2020       4      4  2020-04-01   32656.0000   \n",
       "124      MacBook           4  2020       5      5  2020-05-01   32794.0000   \n",
       "..           ...         ...   ...     ...    ...         ...          ...   \n",
       "55        iPhone          55  2024       8      8  2024-08-01  114390.4926   \n",
       "56        iPhone          56  2024       9      9  2024-09-01  122971.7826   \n",
       "57        iPhone          57  2024      10     10  2024-10-01  126492.4327   \n",
       "58        iPhone          58  2024      11     11  2024-11-01  135728.8369   \n",
       "59        iPhone          59  2024      12     12  2024-12-01  135438.9842   \n",
       "\n",
       "          forecast         error  \n",
       "120       0.000000  31116.000000  \n",
       "121   31116.000000  -1325.000000  \n",
       "122   29791.000000   2370.000000  \n",
       "123   32161.000000    495.000000  \n",
       "124   32656.000000    138.000000  \n",
       "..             ...           ...  \n",
       "55   114374.959248     15.533352  \n",
       "56   122945.909131     25.873469  \n",
       "57   126481.840534     10.592166  \n",
       "58   135700.981000     27.855900  \n",
       "59   135439.891673     -0.907473  \n",
       "\n",
       "[180 rows x 9 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sarimax_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8ff9e8-6fd6-4aaf-94e0-d3852fc597d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
